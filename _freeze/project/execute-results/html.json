{
  "hash": "f30b20e8f0f2879e3583a8bb302b9930",
  "result": {
    "markdown": "---\ntitle: \"Projects\"\nsubtitle: \"My past and ongoing projects\"\ntitle-block-banner: true\ntitle-block-banner-color: white\ncomments: false\npage-layout: full\nformat:\n  html: \n    page-layout: full\n    margin-top: 0em\n    margin-bottom: 0em\n    padding-top: 0em\n    padding-bottom: 0em\n    minimal: true\n    smooth-scroll: true\n    fig-responsive: true\n    toc-location: left\n    toc-depth: 5\n    toc-title: Awesome me\n    number-sections: true  ## add number in the session or not\n    number-depth: 4\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-overflow: wrap\n    code-tools: true\n    code-copy: true\n    highlight: tango\n    df-print: paged  ## kable is another option How the dataframe looks like \n    standalone: false  ### specifies if all assets and libraries must be integrated into the output html file as a standalone document.\n    fig-align: right\n    theme:  solar\ngeometry: margin=lin  \n#execute:\n#  freeze: true  # never re-render during project render\n---\n\n\nThis page contains a brief overview of projects that I significantly shaped throughout the entire project life cycle. In academic terms, this mostly corresponds to first-author publications (single and shared). If you're interested in a full list of projects I have been involved in, please check out my [CV](CV.qmd).\n\n------------------------------------------------------------------------\n\n## Consistency Model Posterior Estimation {#cmpe}\n\n<img src=\"/img/cmpe_banner.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://arxiv.org/abs/2312.05440&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://arxiv.org/abs/2312.05440\" target=\"_blank\">Preprint (arXiv)</a>\n\nConsistency models for neural posterior estimation (CMPE) is a new free-form conditional sampler for scalable, fast, and amortized simulation-based inference with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem.\n\n------------------------------------------------------------------------\n\n## Deep Fusion for Multimodal Simulation-Based Inference {#multinpe}\n\n<img src=\"/img/multi_npe_banner.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://arxiv.org/abs/2311.10671&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://arxiv.org/abs/2311.10671\" target=\"_blank\">Preprint (arXiv)</a>\n\nWe present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy and better performance under partially missing data.\n\n------------------------------------------------------------------------\n\n## Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses\n\n<img src=\"/img/self_consistency_banner.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://openreview.net/forum?id=uoUOz427RD&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://openreview.net/forum?id=uoUOz427RD\" target=\"_blank\">Short Paper (NeurIPS UniReps 2023)</a>\n\nWe propose a method to improve the efficiency and accuracy of amortized Bayesian inference by leveraging universal symmetries in the probabilistic joint model $p(\\theta,y)$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators.\n\n## Meta-Uncertainty in Bayesian Model Comparison {#meta-uncertainty-BMC}\n\n<img src=\"/img/meta_uncertainty_banner.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://arxiv.org/abs/2210.07278&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://proceedings.mlr.press/v206/schmitt23a.html\" target=\"_blank\">Paper (AISTATS 2023)</a> \\| <a href=\"https://github.com/marvinschmitt/MetaUncertaintyPaper\" target=\"_blank\">Code</a> \\| '<a href=\"https://meta-uncertainty.github.io/\" target=\"_blank\">Project website</a> \\| <a href=\"/assets/meta_uncertainty_poster_aistats.pdf\" target=\"_blank\">Poster</a> \\| <a href=\"https://www.youtube.com/watch?v=WIigoUaqy9c\" target=\"_blank\">Presentation (15min)</a><br>\n\nMeta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies.\n\n------------------------------------------------------------------------\n\n## BayesFlow: Amortized Bayesian Workflows With Neural Networks\n\n<img src=\"/img/bayesflow_overview.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://bayesflow.org&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://joss.theoj.org/papers/10.21105/joss.05702\" target=\"_blank\">Software Paper</a> \\| <a href=\"https://bayesflow.org\" target=\"_blank\">Documentation</a> \\| <a href=\"https://discuss.bayesflow.org\" target=\"_blank\">BayesFlow Forums (new!)</a>\n\nBayesFlow is a Python library for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.\n\n------------------------------------------------------------------------\n\n## JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models\n\n<img src=\"/img/jana_figure_1.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://arxiv.org/abs/2302.09125&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://arxiv.org/abs/2302.09125\" target=\"_blank\">Paper (UAI 2023)</a> \\| <a href=\"https://github.com/stefanradev93/BayesFlow\" target=\"_blank\">Python library</a><br>\n\nJANA is a new amortized solution for intractable likelihood functions and posterior densities in Bayesian modeling. It trains three networks to learn both an approximate posterior and a surrogate model for the likelihood, enabling amortized marginal likelihood and posterior predictive estimation.\n\n------------------------------------------------------------------------\n\n## Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks {#sbi-model-misspecification}\n\n<img src=\"/img/model_misspecification_amortized_sbi.png\" style=\"height: 100%; width: 100%; object-fit: contain\" onclick=\"window.open(&apos;https://arxiv.org/abs/2112.08866&apos;, &apos;blank&apos;);\"/>\n\n<a href=\"https://www.dagm-gcpr.de/fileadmin/dagm-gcpr/pictures/2023_Heidelberg/Paper_MainTrack/030.pdf\" target=\"_blank\">Paper (GCPR 2023, Best Paper Honorable Mention)</a> \\| <a href=\"https://github.com/marvinschmitt/ModelMisspecificationBF\" target=\"_blank\">Code</a> \\| <a href=\"/assets/poster_bayescomp_mms.pdf\" target=\"_blank\">Poster</a><br> Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic domain shifts during inference.\n\n\n::: {.cell .styled-output}\n\n```{.r .cell-code}\n#don't run code, but show code\n3\n```\n:::\n\n::: {.cell .styled-output}\n\n```{.r .cell-code}\n#run code, and show code, don't show output\n4\n```\n:::\n\n::: {.cell .styled-output}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#run code, and show code, and show output\n5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n\n```{.r .cell-code}\ns<-function(x){x+1}\ns(23)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24\n```\n:::\n:::\n\n\nFOR REFERENCE: `<svg aria-label=\"Graduation Cap\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#98A08D;overflow:visible;position:relative;\"><title>Graduation Cap</title><path d=\"M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z\"/></svg>`{=html} \\# stopped working 5/1/2023?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}