---
title: "Projects"
subtitle: "My past and ongoing projects"
title-block-banner: true
title-block-banner-color: white
comments: false
page-layout: full
format:
  html: 
    page-layout: full
    margin-top: 0em
    margin-bottom: 0em
    padding-top: 0em
    padding-bottom: 0em
    minimal: true
    smooth-scroll: true
    fig-responsive: true
    toc-location: left
    toc-depth: 5
    toc-title: Awesome me
    number-sections: true  ## add number in the session or not
    number-depth: 4
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-tools: true
    code-copy: true
    highlight: tango
    df-print: paged  ## kable is another option How the dataframe looks like 
    standalone: false  ### specifies if all assets and libraries must be integrated into the output html file as a standalone document.
    fig-align: right
    theme:  solar
geometry: margin=lin  
#execute:
#  freeze: true  # never re-render during project render
---

This page contains a brief overview of projects that I significantly shaped throughout the entire project life cycle. In academic terms, this mostly corresponds to first-author publications (single and shared). If you're interested in a full list of projects I have been involved in, please check out my [CV](CV.qmd).

------------------------------------------------------------------------

## Consistency Model Posterior Estimation {#cmpe}

<img src="/img/cmpe_banner.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://arxiv.org/abs/2312.05440&apos;, &apos;blank&apos;);"/>

<a href="https://arxiv.org/abs/2312.05440" target="_blank">Preprint (arXiv)</a>

Consistency models for neural posterior estimation (CMPE) is a new free-form conditional sampler for scalable, fast, and amortized simulation-based inference with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem.

------------------------------------------------------------------------

## Deep Fusion for Multimodal Simulation-Based Inference {#multinpe}

<img src="/img/multi_npe_banner.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://arxiv.org/abs/2311.10671&apos;, &apos;blank&apos;);"/>

<a href="https://arxiv.org/abs/2311.10671" target="_blank">Preprint (arXiv)</a>

We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy and better performance under partially missing data.

------------------------------------------------------------------------

## Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses

<img src="/img/self_consistency_banner.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://openreview.net/forum?id=uoUOz427RD&apos;, &apos;blank&apos;);"/>

<a href="https://openreview.net/forum?id=uoUOz427RD" target="_blank">Short Paper (NeurIPS UniReps 2023)</a>

We propose a method to improve the efficiency and accuracy of amortized Bayesian inference by leveraging universal symmetries in the probabilistic joint model $p(\theta,y)$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators.

## Meta-Uncertainty in Bayesian Model Comparison {#meta-uncertainty-BMC}

<img src="/img/meta_uncertainty_banner.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://arxiv.org/abs/2210.07278&apos;, &apos;blank&apos;);"/>

<a href="https://proceedings.mlr.press/v206/schmitt23a.html" target="_blank">Paper (AISTATS 2023)</a> \| <a href="https://github.com/marvinschmitt/MetaUncertaintyPaper" target="_blank">Code</a> \| '<a href="https://meta-uncertainty.github.io/" target="_blank">Project website</a> \| <a href="/assets/meta_uncertainty_poster_aistats.pdf" target="_blank">Poster</a> \| <a href="https://www.youtube.com/watch?v=WIigoUaqy9c" target="_blank">Presentation (15min)</a><br>

Meta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies.

------------------------------------------------------------------------

## BayesFlow: Amortized Bayesian Workflows With Neural Networks

<img src="/img/bayesflow_overview.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://bayesflow.org&apos;, &apos;blank&apos;);"/>

<a href="https://joss.theoj.org/papers/10.21105/joss.05702" target="_blank">Software Paper</a> \| <a href="https://bayesflow.org" target="_blank">Documentation</a> \| <a href="https://discuss.bayesflow.org" target="_blank">BayesFlow Forums (new!)</a>

BayesFlow is a Python library for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.

------------------------------------------------------------------------

## JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models

<img src="/img/jana_figure_1.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://arxiv.org/abs/2302.09125&apos;, &apos;blank&apos;);"/>

<a href="https://arxiv.org/abs/2302.09125" target="_blank">Paper (UAI 2023)</a> \| <a href="https://github.com/stefanradev93/BayesFlow" target="_blank">Python library</a><br>

JANA is a new amortized solution for intractable likelihood functions and posterior densities in Bayesian modeling. It trains three networks to learn both an approximate posterior and a surrogate model for the likelihood, enabling amortized marginal likelihood and posterior predictive estimation.

------------------------------------------------------------------------

## Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks {#sbi-model-misspecification}

<img src="/img/model_misspecification_amortized_sbi.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open(&apos;https://arxiv.org/abs/2112.08866&apos;, &apos;blank&apos;);"/>

<a href="https://www.dagm-gcpr.de/fileadmin/dagm-gcpr/pictures/2023_Heidelberg/Paper_MainTrack/030.pdf" target="_blank">Paper (GCPR 2023, Best Paper Honorable Mention)</a> \| <a href="https://github.com/marvinschmitt/ModelMisspecificationBF" target="_blank">Code</a> \| <a href="/assets/poster_bayescomp_mms.pdf" target="_blank">Poster</a><br> Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic domain shifts during inference.

```{r }
#| eval: false
#| echo: true
#| warning: false
#| error: true
#| classes: styled-output
#don't run code, but show code
3
```

```{r}
#| eval: true
#| echo: true
#| output: false
#| classes: styled-output
#run code, and show code, don't show output
4
```

```{r}
#| eval: true
#| echo: false
#| output: true
#| classes: styled-output
#run code, not show code, but show output
1+1
s<-function(x){x+1}
s(2)
```

```{r}
#| eval: true
#| echo: true
#| output: true
#run code, and show code, and show output
5
s<-function(x){x+1}
s(23)
```

FOR REFERENCE: `r fontawesome::fa("graduation-cap", fill = "#98A08D", a11y = "sem")` \# stopped working 5/1/2023?
