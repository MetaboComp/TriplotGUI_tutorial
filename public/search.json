[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "A great person indeed\n\n\n Role: Main contributor of the package and manuscript, slayer of dragon\n Title: PhD candidate\n Affiliation: Food and nutrition science division, department of Life science, Chalmers University of Technology\n Brief:\n Email:  yingxiao@chalmers.se"
  },
  {
    "objectID": "about.html#fa-person-rifle-titleshooter-fa-bullseye-titlearchery-yingxiao-yan",
    "href": "about.html#fa-person-rifle-titleshooter-fa-bullseye-titlearchery-yingxiao-yan",
    "title": "About us",
    "section": "",
    "text": "A great person indeed\n\n\n Role: Main contributor of the package and manuscript, slayer of dragon\n Title: PhD candidate\n Affiliation: Food and nutrition science division, department of Life science, Chalmers University of Technology\n Brief:\n Email:  yingxiao@chalmers.se"
  },
  {
    "objectID": "about.html#fa-chalkboard-user-titleacademic-fa-people-group-titlea-person-with-a-backpack-and-trekking-pol-carl-brunius",
    "href": "about.html#fa-chalkboard-user-titleacademic-fa-people-group-titlea-person-with-a-backpack-and-trekking-pol-carl-brunius",
    "title": "About us",
    "section": "\n  Carl Brunius",
    "text": "Carl Brunius\n\n\n\n\nKnivsta inhabitor\n\n\n Role: Supervisor of Yan\n Title: Associate professor\n Affiliation: Food and nutrition science division, department of Life science, Chalmers University of Technology\n Brief:\n Email:  carl.brunius@chalmers.se"
  },
  {
    "objectID": "about.html#fa-floppy-disk-titleacademic-anton-ribbenstedt",
    "href": "about.html#fa-floppy-disk-titleacademic-anton-ribbenstedt",
    "title": "About us",
    "section": "\n Anton Ribbenstedt",
    "text": "Anton Ribbenstedt\n\n\n\n\nhair length changes all the time\n\n\n Role: code helper and guider\n Title: Research engineer\n Affiliation: Chalmers Mass Spectrometry Infrastructure, Department of Life science, Chalmers university of Technology\n Brief:\n Email:  anton.ribbenstedt@chalmers.se"
  },
  {
    "objectID": "explnation.html",
    "href": "explnation.html",
    "title": "explanation",
    "section": "",
    "text": "Scss rule explain\nThe index main page\nCV page\n.embed-container iframe\n.embed-container:\nThis is a class selector targeting an HTML element with the class name embed-container. It is likely used as a container for embedding various types of content.\niframe: This is an HTML element selector targeting the iframe element.\nCombined: .embed-container iframe: This selector targets any iframe element that is a descendant (child, grandchild, etc.) of an element with the class embed-container.\nThe  element in HTML is used to embed external resources, such as images, audio, videos, or other multimedia content, into a web page. It provides a generic container that allows you to include various types of content within your HTML document.\n&lt;object data=\"example.pdf\" type=\"application/pdf\" width=\"500\" height=\"600\"&gt; Your browser does not support embedded PDF files. &lt;a href=\"example.pdf\"&gt;Download the PDF&lt;/a&gt; instead. &lt;/object&gt;\nThe &lt;iframe&gt; element in HTML (Inline Frame) is used to embed another HTML document within the current document. It allows you to display content from another source, such as a different webpage, in a specified rectangular area on your page. The content inside the &lt;iframe&gt; is essentially a separate document with its own HTML, CSS, and JavaScript.\n&lt;iframe src=\"https://www.example.com\" width=\"600\" height=\"400\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;\nThe &lt;embeded&gt;Used to embed external applications or plugins.\n&lt;embed src=\"example.swf\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"300\"&gt;\n#toolbar=0 This fragment is typically used with PDFs to control the visibility of the PDF viewer’s toolbar. Setting it to 0 generally means hiding the toolbar. The toolbar in a PDF viewer often contains options for navigation, zooming, and other related actions.\nThe height: 0; CSS property is used to set the height of an element to zero pixels. When applied to an element, this property essentially makes the element invisible in terms of its vertical dimension. The content inside the element, if any, will still exist but will be collapsed or hidden.\nHere’s a common use case for height: 0; along with other properties:\nThe CSS property overflow: hidden; is used to control how content that overflows the content area of an element should be handled. When applied to an element, it hides any content that exceeds the specified dimensions of the element’s box.\n\n\n\n \n\n \n\n  \n    © 2024 Yingxiao Yan CC BY-SA 4.0   \n    TriplotGUI gitlab\n    View source of this site on GitLab"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TriplotGUI",
    "section": "",
    "text": "This page gives a brief explanation of what the items in the navigation bar mean.\n\n\nSetup\nThis section gives instructions on how to install the packages needed, load library and activate the user interface.\n\n\nIntroduction\nThis section explains why we develop the TriplotGUI package: The research background, previous work of triplot and our aim.\n\n\nWorkflow\nThis sections gives detailed explanation in statistics and analysis of TriplotGUI\n\n\nTutorial(simple)\nThis section provides a simple example of how to use the TriplotGUI package. The users can either use code or the user interface to achieve the similar purpose. The code tutorial suits for users that is at beginner R level. The user interface tutorial suits for user that has limited epidemiology knowledge and limited experience in dealing with high dimensional data.\nThe dataset used is CAMP_2\n\n\nTutorial(complex)\nThis section provides a simple example of how to use the TriplotGUI package. The users can either use code or the user interface to achieve the similar purpose. The code tutorial suits for users that is at intermediate or advanced R level. The user interface tutorial suits for user that has epidemiology knowledge and experience in dealing with high dimensional data.\nThe dataset used is HealthyNordicDiet_2.\n\n\nUse Case\nThis section shows a smoother example of how to use the interface to perform data analysis and interpretation, omiting the detailed explanation in the tutorial.\nThe dataset used is ExposomeChallenge.\n\n\nPaper\nThis sections covers relevant papers of our TriplotGUI package\n\n\nAbout us\nThis sections give brief introduction of the contributors to this package.\n\n\nContact us\nIf you have questions in the package, tutorial and this website, or would like to seek collaboration, please contact yingxiao@chalmers.se"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Install packages\nPlease run the following code to install the packages needed. This needs to be run in your R console.\n\nCode### I may need mix Omics and impute not sure\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"mixOmics\")\nBiocManager::install(\"impute\")\nBiocManager::install(\"preprocessCore\")\nBiocManager::install(\"GO.db\")\n\ninstall.packages(\"remotes\")\nremotes::install_gitlab(\"YingxiaoYan/TriplotGUI\")\n\n\nThings may pop up in your console and ask you for some choices, we provide some recommedations here. We are conservative in the choices to avoid any unforseen conflict and complications in packages.\n\n\nSometimes it pops up in your console to ask you which package you would like to update. We recommened you to chose 3.None\n\n\nSometimes it pops up in your console to ask Do you want to install from soutce packages which need compliation . We recommended you to chose no\n\n\nSometimes it pops up in your console to ask Update all/some/none?[a/s/n] . We recommended you to chose n\n\nLoad libraries\nRun the following code in your R console.\n\nCodelibrary(TriplotGUI)\n\n\nDownload the example data for user interface\nGo to my git repository. Click the download icon at the right side and download the Exampledata.rar. Unzip it and you will find the data that will be used as example in the tutorial.\nChange the maximize size of data file allowed\nThe default of our package has allows data of maximize size of 5 MB. To change that, you could specify the maximize file size you prefer using the code below. Run your code in the console. However, we do not recommend to use data with more than 5 MB in our tool. The bigger the dataset, the slower it will be. It might be beneficial for you to do some variable selection before putting the data in out tool.\n\nCodefilesize=100  ##MB\noptions(shiny.maxRequestSize=filesize*1024^2)\n\n\nActivate the user interface\nRun the following code in your R console.\n\nCodeTriplotGUI_shiny_tryout()"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Overview\n   Most observational epidemiology research studies associations between single exposure and outcome. Emerging exposure- and outcome-wide studies aim to more broadly identify potential risk factors and their health effects. Although omics technologies have permitted linking exposures to outcomes via molecular data, large-scale exploration of mediating mechanisms is frequently lacking. Likely because Omics data are often high dimensional and there is a lack of effective tools for direct interpretation of the complex relationships between multiple exposures, Omics and outcomes. We therefore developed the TriplotGUI tool to advance exposome-risk assessment and facilitate the visualization and interpretation of such complex associations via metabolic regulation.\nThe application adopted a highly reactive stepwise modular design, where one or two modules represents one of the 6 following steps and the change of input from an early step will feed into a later step.\n \n\n\nSteps\nStep1: Data reduction of omics data\nTransforms the Omics data into a number of components using principal component analysis (PCA) or weighted correlation network analysis (WGCNA).\nStep2A: Exposures’ correlations\nAssesses correlations between exposures and component scores from Step 1. Confounders can be adjusted.\nStep2B: Outcomes’ associations\nAssesses risk associations between outcomes and the component scores from Step 1. Confounders can be adjusted.\nStep3: Visualization of Triplot\nThe component scores and loadings, the correlation coefficients and the risk associations’ estimates as beta coefficients or odds ratio, are co-visualized as 3 different layers in one 2-dimensional plot.\nStep4: Mediation analysis and visualization\nPerforms mediation analysis on the selected exposures and outcomes, using component scores as mediators. The mediation estimates (i.e. direct, indirect and total effect) or the proportion mediated are then co-visualized with the correlation coefficients of the selected exposures and the risk associations’ estimates as beta coefficients or odds ratio of the selected outcomes as 3 different layers in one 2-dimensional plot.\nStep5: Compare correlations, associations and mediations\nThe correlations, associations, mediation estimates and proportion mediated as well as their significance levels can be visualized through heatmap.\nStep6: Download data\nThe intermediate data and the generated results can be viewed and downloaded. The object that saves all relevant information can be downloaded.\n We will give detailed explanations of statistical data analysis and visualization in each step in the workflow session."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Relevant papers",
    "section": "",
    "text": "Download main paper \n\n\n\n\nMain paper\n\n\n\nSchillemans T, Shi L, Liu X, Åkesson A, Landberg R, Brunius C. Visualization and Interpretation of Multivariate Associations with Disease Risk Markers and Disease Risk-The Triplot. Metabolites. 2019 Jul 6;9(7):133. doi: 10.3390/metabo9070133\n\n\n\n\n\n\nOther papers\n\nBodén S, Zheng R, Ribbenstedt A, Landberg R, Harlid S, Vidman L, Gunter MJ, Winkvist A, Johansson I, Van Guelpen B, Brunius C. Dietary patterns, untargeted metabolite profiles and their association with colorectal cancer risk. Sci Rep. 2024 Jan 26;14(1):2244. doi: 10.1038/s41598-023-50567-6\nSchillemans T, Yan Y, Ribbenstedt A, Donat-Vargas C, Lindh CH, Kiviranta H, Rantakokko P, Wolk A, Landberg R, Åkesson A, Brunius C. OMICs Signatures Linking Persistent Organic Pollutants to Cardiovascular Disease in the Swedish Mammography Cohort. Environ Sci Technol. 2024 Jan 16;58(2):1036-1047. doi: 10.1021/acs.est.3c06388.\nTitova OE, Brunius C, Warensjö Lemming E, Stattin K, Baron JA, Byberg L, Michaëlsson K, Larsson SC. Comprehensive analyses of circulating cardiometabolic proteins and objective measures of fat mass. Int J Obes (Lond). 2023 Nov;47(11):1043-1049. doi: 10.1038/s41366-023-01351-z"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a brief overview of projects that I significantly shaped throughout the entire project life cycle. In academic terms, this mostly corresponds to first-author publications (single and shared). If you’re interested in a full list of projects I have been involved in, please check out my CV."
  },
  {
    "objectID": "project.html#cmpe",
    "href": "project.html#cmpe",
    "title": "Projects",
    "section": "\n1 Consistency Model Posterior Estimation",
    "text": "1 Consistency Model Posterior Estimation\n\nPreprint (arXiv)\nConsistency models for neural posterior estimation (CMPE) is a new free-form conditional sampler for scalable, fast, and amortized simulation-based inference with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem."
  },
  {
    "objectID": "project.html#multinpe",
    "href": "project.html#multinpe",
    "title": "Projects",
    "section": "\n2 Deep Fusion for Multimodal Simulation-Based Inference",
    "text": "2 Deep Fusion for Multimodal Simulation-Based Inference\n\nPreprint (arXiv)\nWe present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy and better performance under partially missing data."
  },
  {
    "objectID": "project.html#data-efficient-amortized-bayesian-inference-via-self-consistency-losses",
    "href": "project.html#data-efficient-amortized-bayesian-inference-via-self-consistency-losses",
    "title": "Projects",
    "section": "\n3 Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses",
    "text": "3 Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses\n\nShort Paper (NeurIPS UniReps 2023)\nWe propose a method to improve the efficiency and accuracy of amortized Bayesian inference by leveraging universal symmetries in the probabilistic joint model \\(p(\\theta,y)\\). In a nutshell, we invert Bayes’ theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators."
  },
  {
    "objectID": "project.html#meta-uncertainty-BMC",
    "href": "project.html#meta-uncertainty-BMC",
    "title": "Projects",
    "section": "\n4 Meta-Uncertainty in Bayesian Model Comparison",
    "text": "4 Meta-Uncertainty in Bayesian Model Comparison\n\nPaper (AISTATS 2023) | Code | ’Project website | Poster | Presentation (15min)\nMeta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies."
  },
  {
    "objectID": "project.html#bayesflow-amortized-bayesian-workflows-with-neural-networks",
    "href": "project.html#bayesflow-amortized-bayesian-workflows-with-neural-networks",
    "title": "Projects",
    "section": "\n5 BayesFlow: Amortized Bayesian Workflows With Neural Networks",
    "text": "5 BayesFlow: Amortized Bayesian Workflows With Neural Networks\n\nSoftware Paper | Documentation | BayesFlow Forums (new!)\nBayesFlow is a Python library for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized."
  },
  {
    "objectID": "project.html#jana-jointly-amortized-neural-approximation-of-complex-bayesian-models",
    "href": "project.html#jana-jointly-amortized-neural-approximation-of-complex-bayesian-models",
    "title": "Projects",
    "section": "\n6 JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models",
    "text": "6 JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models\n\nPaper (UAI 2023) | Python library\nJANA is a new amortized solution for intractable likelihood functions and posterior densities in Bayesian modeling. It trains three networks to learn both an approximate posterior and a surrogate model for the likelihood, enabling amortized marginal likelihood and posterior predictive estimation."
  },
  {
    "objectID": "project.html#sbi-model-misspecification",
    "href": "project.html#sbi-model-misspecification",
    "title": "Projects",
    "section": "\n7 Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks",
    "text": "7 Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks\n\nPaper (GCPR 2023, Best Paper Honorable Mention) | Code | Poster Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic domain shifts during inference.\n\nCode#don't run code, but show code\n3\n\n\n\nCode#run code, and show code, don't show output\n4\n\n\n\n\n[1] 2\n\n\n[1] 3\n\n\n\nCode#run code, and show code, and show output\n5\n\n[1] 5\n\nCodes&lt;-function(x){x+1}\ns(23)\n\n[1] 24\n\n\nFOR REFERENCE: Graduation Cap\n # stopped working 5/1/2023?"
  },
  {
    "objectID": "pub_ho.html",
    "href": "pub_ho.html",
    "title": "Publication & honor",
    "section": "",
    "text": "Luyan Wu, Wenhui Zeng, Liandong Feng, Yuxuan Hu, Yidan Sun, Yingxiao Yan, HongYuan Chen & Deju Ye. An activatable radiometric near-inflared fluorescent probe for hydrogen sulfide imaging in vivo. Science China Chemistry. 2020;63(5):741-750 Yan Y, Smith E, Melander O, Ottosson F. The association between plasma metabolites and future risk of all‐cause mortality. Journal of Internal Medicine. 2022;292(5):804-815.  Schillemans T, Yan Y, Ribbenstedt A, Donat-Vargas C, Lindh C, Kivirante H, Wolk A, et al.OMICs signatures linking persistent organic pollutants to cardiovascular disease in the Swedish Mammography Cohort [Submitted]"
  },
  {
    "objectID": "pub_ho.html#publications",
    "href": "pub_ho.html#publications",
    "title": "Publication & honor",
    "section": "",
    "text": "Luyan Wu, Wenhui Zeng, Liandong Feng, Yuxuan Hu, Yidan Sun, Yingxiao Yan, HongYuan Chen & Deju Ye. An activatable radiometric near-inflared fluorescent probe for hydrogen sulfide imaging in vivo. Science China Chemistry. 2020;63(5):741-750 Yan Y, Smith E, Melander O, Ottosson F. The association between plasma metabolites and future risk of all‐cause mortality. Journal of Internal Medicine. 2022;292(5):804-815.  Schillemans T, Yan Y, Ribbenstedt A, Donat-Vargas C, Lindh C, Kivirante H, Wolk A, et al.OMICs signatures linking persistent organic pollutants to cardiovascular disease in the Swedish Mammography Cohort [Submitted]"
  },
  {
    "objectID": "pub_ho.html#conference-and-awards",
    "href": "pub_ho.html#conference-and-awards",
    "title": "Publication & honor",
    "section": "\n2 Conference and awards",
    "text": "2 Conference and awards\n\n\nMetabolomics 2022, Valencia, Spain. - Speaker, with award\n\n\nSwedish Bioinformatics workshop 2022,Umeå, Sweden. - Speaker\n\n\nNordic rye forum 2022, Stockholm, Sweden - Speaker"
  },
  {
    "objectID": "pub_ho.html#key-competence",
    "href": "pub_ho.html#key-competence",
    "title": "Publication & honor",
    "section": "\n3 Key competence",
    "text": "3 Key competence\n\n\nR/shiny (data cleaning, manipulation, analysis, and visualization) - Expert\n\n\nBiostatistics and epidemiological research - Expert\n\n\nSPSS, Excel & Microsoft Office suite - Expert\n\n\npython - Intermediate"
  },
  {
    "objectID": "pub_ho.html#languages",
    "href": "pub_ho.html#languages",
    "title": "Publication & honor",
    "section": "\n4 Languages",
    "text": "4 Languages\n\n\nMandarin (Native speaker)\n\n\nEnglish (Native level)\n\n\nSwedish (Intermediate/B1 level, SAS certificated)"
  },
  {
    "objectID": "pub_ho.html#references",
    "href": "pub_ho.html#references",
    "title": "Publication & honor",
    "section": "\n5 References",
    "text": "5 References\nFilip Ottoson Postdoctoral Researcher – Lund University Email: filip.ottoson@med.lu.se Tel: +46739699000 Carl Brunius Associate Professor – Chalmers University of Technology Email: calbrunius@chalmers.se Tel:\n\n© 2023 Yingxiao Yan ★★★★★. All rights reserved"
  },
  {
    "objectID": "pub_ho.html#news",
    "href": "pub_ho.html#news",
    "title": "Publication & honor",
    "section": "\n6 News",
    "text": "6 News\n\nJanuary 23, 2024: I will be at Bayes on the Beach 2024 in February. I will give a talk about reliable amortized Bayesian inference with neural networks and co-lead a workshop on amortized Bayesian workflows.\nDecember 2023: We launched the BayesFlow Forums! The BayesFlow Forums provide a community for asking and answering questions about all aspects of BayesFlow and amortized Bayesian workflows in general. Hop over and join the discussion!\nOctober 11, 2023: In our recent preprint, we make simulation-based inference more data-efficient by leveraging self-consistency properties of the Bayesian joint model. You find the preprint on arXiv. Update: Published in the NeurIPS UniReps workshop.\nSeptember 22, 2023: Our paper “Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks” has been awarded the DAGM GCPR Honorable Mention at this year’s German Conference on Pattern Recognition. Huge thanks to my great co-authors!\nSeptember 6, 2023: Our paper “Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks” has been accepted with oral presentation at the German Conference for Pattern Recognition 2023. Learn more at the project website.\nJune 21, 2023: I will be at the ELLIS Doctoral Symposium 2023 in Helsinki from August 28 to September 1, 2023. Absolutely thrilled to meet other PhD students and researchers and talk about machine learning research in beautiful Helsinki!\nMay 9, 2023: Our paper “JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models” has been accepted to UAI 2023 (Uncertainty in AI). Check out the project page to find out more!\nMarch 29, 2023: I’ll be presenting our paper on Meta-Uncertainty in Bayesian Model Comparison at AISTATS 2023 from April 25-27 in Valencia, Spain. You find links to the paper and code on my Projects page or on the dedicated paper website. If you see me around at AISTATS, let’s chat!"
  },
  {
    "objectID": "pub_ho.html#featured-blog-posts",
    "href": "pub_ho.html#featured-blog-posts",
    "title": "Publication & honor",
    "section": "\n7 Featured Blog Posts",
    "text": "7 Featured Blog Posts\n\n\n\n\nCode#don't run code, but show code\n3\n\n\n\nCode#run code, and show code, don't show output\n4\n\n\n\n\n[1] 2\n\n\n\nCode#run code, and show code, and show output\n5\n\n[1] 5"
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "We will go through the statistical analysis behind TriplotGUI and also explain the rationale of our visulaization in each step. If you want to search the meaning of something specific (e.g. a button, a box), please press ctrl+F and enter the keyword, you will probably find what you need.\nIf you have not open the app yet, follow what is written in the Setup and open the interface with TriplotGUI_shiny()"
  },
  {
    "objectID": "workflow.html#input-data",
    "href": "workflow.html#input-data",
    "title": "Workflow",
    "section": "\n1.1 Input data",
    "text": "1.1 Input data"
  },
  {
    "objectID": "workflow.html#analysis",
    "href": "workflow.html#analysis",
    "title": "Workflow",
    "section": "\n1.2 Analysis",
    "text": "1.2 Analysis"
  },
  {
    "objectID": "workflow.html#analysis-settings",
    "href": "workflow.html#analysis-settings",
    "title": "Workflow",
    "section": "\n1.3 Analysis settings",
    "text": "1.3 Analysis settings"
  },
  {
    "objectID": "workflow.html#visualizations",
    "href": "workflow.html#visualizations",
    "title": "Workflow",
    "section": "\n1.5 Visualizations",
    "text": "1.5 Visualizations\nThe green box represents visualization outputs. With omics data uploaded, upon selecting Which plots in the data visualization setting, the corresponding plot are shown."
  },
  {
    "objectID": "workflow.html#visualization-settings",
    "href": "workflow.html#visualization-settings",
    "title": "Workflow",
    "section": "\n1.4 Visualization settings",
    "text": "1.4 Visualization settings\nThe orange box is where users can modify their visulization outputs\n\n1.4.1 Basics\n\nWhich plots: Four options are provided: (1) scores: The scores of principal components are plotted. (2)loadings: The loadings of principal components are plotted. (3) biplots: Both scores and loadings are plotted on the same plot. (4) screeplot: How much variance does each principal component contribute to is plotted. In default mode, none of the 4 options are selected\nComponent/module on first axis: The principal component that will be plotted on the x-axis. The default is 1 (the first one).\nComponent/module on second axis: The principal component that will be plotted on the y-axis. The default is 2 (the second one).\nWhich type of loading to use Two options are provided Eigenvectors and Eigenvectors × squareroot(eigenvalues). The 2 calulations, respectively, correspond to what is referred to as “rotation” in the prcomp package and “Loadings” in the principal package\n\n1.4.2 Emerged\n\nLoading labels: Upon clicking loadings or biplot in Which plots. The button will show up. It decides if labels of loadings will be shown on the plot. The default is Yes\nScale biplot: Upon clicking biplot in Which plots. The button will show up. It decides if scores and loadings will be both scaled to a visible level by adjusting the limit of axises. The default is Yes\nCut ratio or absolute value: Upon clicking loadings or biplot in Which plots. The button will show up. It is useful when the users only want to show loadings bigger than a certain value or bigger than a certain percent of the maximum loading. The default is Absolute value\nLoadings cut absolute value: When selecting Absolute value in Cut ratio or absolute value, a slider will show up to let user decide the loadings below what value will not show up on the plot\nLoadings cut ratio: When selecting Cut ratio in Cut ratio or absolute value, a slider will show up to let user decide the loadings below how much percent of the maximum loading will not show up on the plot\nScore color by: Upon uploading the Auxilary data and clicking loadings or biplot in Which plots, this button will show up. Users can use one numeric or categorical variable in the auxilary data to color the scores points. By default, no variables are selected.\nScore shape by: Upon uploading the Auxilary data and clicking loadings or biplot in Which plots, this button will show up. Users can use one categorical variable in the auxilary data to change the shape of scores points. By default, no variables are selected.\nScore size by: Upon uploading the Auxilary data and clicking loadings or biplot in Which plots, this button will show up. Users can use one numeric variable in the auxilary data to change the size of score points. By default, no variables are selected."
  },
  {
    "objectID": "workflow.html#data-for-pcawgcna",
    "href": "workflow.html#data-for-pcawgcna",
    "title": "Workflow",
    "section": "\n1.1 Data for PCA/WGCNA",
    "text": "1.1 Data for PCA/WGCNA\n\n1.1.1 Basics\nThe dark blue box is where you put in the Omics data. The data should meet the following criteria.\n\nThe data should be in either csv, xlsx, rds, or rda format. The data in rds format should be a dataframe. If the data is in rda format, the rda object should only contain the data as a dataframe and be in the same directory as your working directory (You could check this by getwd() in the R console)\nThe rows of data should represent observations and the columns of data should represents variables(omics features).\nNo missing values are allowed in the data\nThe number and order of observations should be same across all the uploaded data\n\n\n\n\n\n\n\nNote\n\n\n\nWe do not provide the functionality of handling missing data or other data pre-processing steps (e.g. transformation, normalization) in our application due to the variability of possible methods.\n\n\n\n1.1.2 Emerged\nAfter you upload the data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Force all variables to numeric.\n\nInspect variable class This is a button that help you inspect if the variables are in correct format, since you may not want your metabolite features in character format when you want to use their numeric values. Click the button and explore.\n\nremove variables: This button allows you to remove variables from your uploaded data. Click the button, remove some variables and click OK. What variables are removed are then printed out and they will not be included in the Omics data to enter the downstream analysis. We also make this removal reversible so that every time when you reclick this button, the data will go back to the state where no variables are removed.\nWe add this button so that users will not need to manually remove redundant variables that will actually not be used in data analysis.\n\nForce all variables to numeric: Upon clicking, this button simply forces all the variables to numeric. Since we want to perform PCA or WGCNA on this Omics data, we want all Omics variables to be in numeric format\n\n1.1.3 And more\nAt the right side of the upload data, there is a button called Reset everything. This where you can click remove all your uploads and settings and make the interface go back to a default state."
  },
  {
    "objectID": "workflow.html#auxilary-data",
    "href": "workflow.html#auxilary-data",
    "title": "Workflow",
    "section": "\n1.2 Auxilary data",
    "text": "1.2 Auxilary data\nThe light blue box is where you put in the auxilary data. Currently, the variables in the data uploaded here can be used for 2 things (1) Customizing the color, size and shape of scores. See section 1.4.2. (2) Providing pairing information for outcomes’ associations. See section 1.1.1.\n\n1.2.1 Basics\nThe requirement for input data is the same as 1.1.1.\n\n1.2.2 Emerged\nAfter you upload the axuliary data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\n\nChange variable class: This button provide your choices in transforming variables to either factor and numeric variables.\nWe add this button to faciliate user in tranforming variables to numeric and factor format, corresponding to numeric and caregorical variables."
  },
  {
    "objectID": "workflow.html#data-analysis-settings",
    "href": "workflow.html#data-analysis-settings",
    "title": "Workflow",
    "section": "\n1.3 Data analysis settings",
    "text": "1.3 Data analysis settings\nThe red box is where you make choices in how do you want your data analysis to be performed\n\n1.3.1 Basics\n\nMethod: Users choose to either use principal component analysis (PCA) or weighted correlation network analysis (WGCNA) to perform data reduction. The default is PCA\nPCA function: Users choose what PCA function, prcomp or principal, will be used for principal component analysis. The default is prcomp\n\nMax number of components: This button shows up when the Method is PCA. It decides how many principal component will be used in the downstream correlation& association analysis. The default is set as around the half of the total number of omics variables used for analysis.\nMinimum number for variables per module: This button shows up when the Method is WGCNA. It decides the minimum number of variables that is allowed in each cluster of WGCNA. The default number is 2.\n\nCenter: If the omics variables will be zero centered before the analysis take place. The default is Yes.\nScale: If the omics variables will be scaled to have unit variance before the analysis take place. The default is Yes.\n\n1.3.2 Emerged\n\n\nrotate: The button will show up and When PCA function is chosen as principal. How users would like to transform principal components. Please check here for the explanation fo rotate. The default is set as none in TriplotGUI.\n\n\n\n\n\n\n\nNote\n\n\n\nYou maybe heard about PCA but not WGCNA. In brief, WGCNA is to separate omics variables into clusters, perform PCA on each cluster and extract the first principal component of each cluster as the principal components of WGCNA. The clustering method can be different. In TriplotGUI, we use the default hierarchical clustering method provided by cutreeDynamic() in the dunamicTreeCut package"
  },
  {
    "objectID": "workflow.html#auxilary_data",
    "href": "workflow.html#auxilary_data",
    "title": "Workflow",
    "section": "\n1.2 Auxilary data",
    "text": "1.2 Auxilary data\nThe light blue box is where you put in the auxilary data. Currently, the variables in the data uploaded here can be used for 2 things (1) Customizing the color, size and shape of scores. See section 1.4.2. (2) Providing pairing information for outcomes’ associations. See section 1.1.1.\n\n1.2.1 Basics\nThe requirement for input data is the same as 1.1.1.\n\n1.2.2 Emerged\nAfter you upload the axuliary data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\n\nChange variable class: This button provide your choices in transforming variables to either factor and numeric variables.\nWe add this button to faciliate user in tranforming variables to numeric and factor format, corresponding to numeric and caregorical variables."
  },
  {
    "objectID": "workflow.html#exposure-data",
    "href": "workflow.html#exposure-data",
    "title": "Workflow",
    "section": "\n2.1 Exposure data",
    "text": "2.1 Exposure data\n\n2.1.1 Basics\nThe dark blue box is where you put in the exposure data. The requirement for exposure data is the same as 1.1.1.\n\n2.1.2 Emerged\nAfter you upload the exposure data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#covariate-data",
    "href": "workflow.html#covariate-data",
    "title": "Workflow",
    "section": "\n2.2 Covariate data",
    "text": "2.2 Covariate data\nThe light blue box is where you put in the covariate data. After removing redundant variables, the remaining variables can be used as confounders for the partial correlation between exposures and principal components\n\n2.2.1 Basics\nThe requirement for exposure data is the same as 1.1.1.\n\n2.2.2 Emerged\nAfter you upload the covariate data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#data-analysis-settings-1",
    "href": "workflow.html#data-analysis-settings-1",
    "title": "Workflow",
    "section": "\n2.3 Data analysis settings",
    "text": "2.3 Data analysis settings\nThe red box is where you make choices in how do you want your correlation analysis to be performed\n\n2.3.1 Basics\n\nMethod: Three methods inherited from the cor package are supported for continuous exposures: pearson, spearman, kendall. Please check cor package for more information. The default for TriplotGUI is pearson.\nManage missing values: The approaches for managing missing values inherited from the cor package are supported for continuous exposures. Please check cor package for more information. The default for TriplotGUI is everything.\nCategorical variables: Two options are provided Perform one-hot-encoding and Use original. When choosing Perform one-hot-encoding, categorical variables with n classes(n&gt;2) are transformed to n binary variables with 0 and 1 values and then used as numeric variables. When chosing Use original, no transformation is made on the categorical variabls and linear models are used for categorical exposure-principal component correlation analysis. The default is Use original.\n\n2.3.2 Emerged\n\n\nFull or partial correaltion Upon uploading the covariate data, the button will show up. Two options are provided: Full and Partial. Full means no covariates are used as confounders in the correlation analysis and Partial means that partial correlations, adjusting the same group of confounders, are performed on all exposures-principal components correlations."
  },
  {
    "objectID": "workflow.html#visualizations-1",
    "href": "workflow.html#visualizations-1",
    "title": "Workflow",
    "section": "\n2.4 Visualizations",
    "text": "2.4 Visualizations\nThe green box represents visualization outputs. With omics data from step 1 and exposure data uploaded, upon clicking the Refresh plot button, the correlations between exposures and the 2 selected components (selected from step 1) are shown. The figure can be downloaded by clicking Download figure."
  },
  {
    "objectID": "workflow.html#Covaraite_data1",
    "href": "workflow.html#Covaraite_data1",
    "title": "Workflow",
    "section": "\n2.2 Covariate data",
    "text": "2.2 Covariate data\nThe light blue box is where you put in the covariate data. After removing redundant variables, the remaining covariate data can be used as confounders for the partial correlation between exposures and principal components\n\n2.2.1 Basics\nThe requirement for exposure data is the same as 1.1.1.\n\n2.2.2 Emerged\nAfter you upload the covariate data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#outcome-data",
    "href": "workflow.html#outcome-data",
    "title": "Workflow",
    "section": "\n3.1 Outcome data",
    "text": "3.1 Outcome data\n\n3.1.1 Basics\nThe dark blue box is where you put in the outcome data. The requirement for exposure data is the same as 1.1.1.\n\n3.1.2 Emerged\nAfter you upload the exposure data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#Covaraite_data2",
    "href": "workflow.html#Covaraite_data2",
    "title": "Workflow",
    "section": "\n3.2 Covariate data",
    "text": "3.2 Covariate data\nThe light blue box is where you put in the covariate data. After removing redundant variables, the remaining covariate data can be used as confounders for the partial correlation between outcomes and principal components\n\n3.2.1 Basics\nThe requirement for exposure data is the same as 1.1.1.\n\n3.2.2 Emerged\nAfter you upload the covariate data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#data-analysis-settings-2",
    "href": "workflow.html#data-analysis-settings-2",
    "title": "Workflow",
    "section": "\n3.3 Data analysis settings",
    "text": "3.3 Data analysis settings\nThe red box is where you make choices in how do you want your risk association analysis to be performed.\n\n\n\n\n\n\nNote\n\n\n\nFor outcomes - principal-components associations, statistical analysis are performed differently based on different classes of outcome and information provided. The table below summarized the methods and R packages used.\n\n\n\n\n3.3.1 Basics\n\n\nlog(risk estimates): Users can choose whether they want their risk estimations to be presented as beta coefficients or odds ratios. Two options are provided Yes (useful for odds ratio) and No (useful for beta coefficient). The default is No (useful for beta coefficient)\n\n\n3.3.2 Emerged\n\nDo confounder adjustment: Upon uploading the covariate data, the button will show up. The users can choose if confounder will be adjusted for all outcomes-principal components associations, using the same group of confounders. The default is No.\nMultinomial: If categorical variables with n&gt;2 class is uploaded or created from outcome variables, the button will show up. By selecting No, one-hot-encoding will be performed on these categorical variables to transform them to n binary variables with 0 and 1 values. By selecting Yes, multinomial regression will be performed on these categorical variables. The default is No\nPairing variable  When auxilary data (1.2.1) is uploaded, this button will show up. It allows users to select variables that contain pairing information in the auxilary data. For example, the matched case-control pair could constitute a categorical variable, where each pair of case and control is given a unique number. Such information can be fed in the modelling of outcome-principal components associations."
  },
  {
    "objectID": "workflow.html#visualization-settings-1",
    "href": "workflow.html#visualization-settings-1",
    "title": "Workflow",
    "section": "\n3.4 Visualization settings",
    "text": "3.4 Visualization settings\nThe orange box is where users can modify their visulization outputs\n\n3.4.1 Basics\n\nConfidence level: The confidence level used when calulating the confidence interval of risk estimate.The default is 0.95.\nWhisker length:The whisker length of confidence intervals in the figures. Change it from 0.01 to 0.5 and see what is changed in Visulizations! The default is 0.01."
  },
  {
    "objectID": "workflow.html#visualizations-2",
    "href": "workflow.html#visualizations-2",
    "title": "Workflow",
    "section": "\n3.5 Visualizations",
    "text": "3.5 Visualizations\nThe green box represents visualization outputs. With omics data from step 1 and outcome data uploaded, upon clicking the Refresh plot button, the risk assocations between outcomes and 2 selected components (selected from step 1) are shown. The figure can be downloaded by clicking Download figure."
  }
]