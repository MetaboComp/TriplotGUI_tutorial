[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "A great person indeed\n\n\n Role: Main contributor of the package and manuscript, slayer of dragon\n Title: PhD candidate\n Affiliation: Food and nutrition science division, department of Life science, Chalmers University of Technology\n Brief:\n Email:  yingxiao@chalmers.se"
  },
  {
    "objectID": "about.html#fa-person-rifle-titleshooter-fa-bullseye-titlearchery-yingxiao-yan",
    "href": "about.html#fa-person-rifle-titleshooter-fa-bullseye-titlearchery-yingxiao-yan",
    "title": "About us",
    "section": "",
    "text": "A great person indeed\n\n\n Role: Main contributor of the package and manuscript, slayer of dragon\n Title: PhD candidate\n Affiliation: Food and nutrition science division, department of Life science, Chalmers University of Technology\n Brief:\n Email:  yingxiao@chalmers.se"
  },
  {
    "objectID": "about.html#fa-chalkboard-user-titleacademic-fa-people-group-titlea-person-with-a-backpack-and-trekking-pol-carl-brunius",
    "href": "about.html#fa-chalkboard-user-titleacademic-fa-people-group-titlea-person-with-a-backpack-and-trekking-pol-carl-brunius",
    "title": "About us",
    "section": "\n  Carl Brunius",
    "text": "Carl Brunius\n\n\n\n\nKnivsta inhabitor\n\n\n Role: Supervisor of Yan\n Title: Associate professor\n Affiliation: Food and nutrition science division, department of Life science, Chalmers University of Technology\n Brief:\n Email:  carl.brunius@chalmers.se"
  },
  {
    "objectID": "about.html#fa-floppy-disk-titleacademic-anton-ribbenstedt",
    "href": "about.html#fa-floppy-disk-titleacademic-anton-ribbenstedt",
    "title": "About us",
    "section": "\n Anton Ribbenstedt",
    "text": "Anton Ribbenstedt\n\n\n\n\nhair length changes all the time\n\n\n Role: code helper and guider\n Title: Research engineer\n Affiliation: Chalmers Mass Spectrometry Infrastructure, Department of Life science, Chalmers university of Technology\n Brief:\n Email:  anton.ribbenstedt@chalmers.se"
  },
  {
    "objectID": "complex_code.html",
    "href": "complex_code.html",
    "title": "Tutorial(complex) - Code",
    "section": "",
    "text": "Code#don't run code, but show code\n3\n\n\n\nCode#run code, and show code, don't show output\n4\n\n\n\n\n[1] 2\n\n\n[1] 3\n\n\n\n\n\n\nCode#run code, and show code, and show output\n5\n\n[1] 5\n\nCodes&lt;-function(x){x+1}\ns(23)\n\n[1] 24\n\n\nFOR REFERENCE: Graduation Cap\n # stopped working 5/1/2023?"
  },
  {
    "objectID": "explnation.html",
    "href": "explnation.html",
    "title": "explanation",
    "section": "",
    "text": "Scss rule explain\nThe index main page\nCV page\n.embed-container iframe\n.embed-container:\nThis is a class selector targeting an HTML element with the class name embed-container. It is likely used as a container for embedding various types of content.\niframe: This is an HTML element selector targeting the iframe element.\nCombined: .embed-container iframe: This selector targets any iframe element that is a descendant (child, grandchild, etc.) of an element with the class embed-container.\nThe  element in HTML is used to embed external resources, such as images, audio, videos, or other multimedia content, into a web page. It provides a generic container that allows you to include various types of content within your HTML document.\n&lt;object data=\"example.pdf\" type=\"application/pdf\" width=\"500\" height=\"600\"&gt; Your browser does not support embedded PDF files. &lt;a href=\"example.pdf\"&gt;Download the PDF&lt;/a&gt; instead. &lt;/object&gt;\nThe &lt;iframe&gt; element in HTML (Inline Frame) is used to embed another HTML document within the current document. It allows you to display content from another source, such as a different webpage, in a specified rectangular area on your page. The content inside the &lt;iframe&gt; is essentially a separate document with its own HTML, CSS, and JavaScript.\n&lt;iframe src=\"https://www.example.com\" width=\"600\" height=\"400\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;\nThe &lt;embeded&gt;Used to embed external applications or plugins.\n&lt;embed src=\"example.swf\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"300\"&gt;\n#toolbar=0 This fragment is typically used with PDFs to control the visibility of the PDF viewer’s toolbar. Setting it to 0 generally means hiding the toolbar. The toolbar in a PDF viewer often contains options for navigation, zooming, and other related actions.\nThe height: 0; CSS property is used to set the height of an element to zero pixels. When applied to an element, this property essentially makes the element invisible in terms of its vertical dimension. The content inside the element, if any, will still exist but will be collapsed or hidden.\nHere’s a common use case for height: 0; along with other properties:\nThe CSS property overflow: hidden; is used to control how content that overflows the content area of an element should be handled. When applied to an element, it hides any content that exceeds the specified dimensions of the element’s box.\n\n\n\n \n\n \n\n  \n    © 2024 Yingxiao Yan CC BY-SA 4.0   \n    TriplotGUI gitlab\n    View source of this site on GitLab"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TriplotGUI",
    "section": "",
    "text": "This page gives a brief explanation of what the items in the navigation bar mean.\n\n\nSetup\nThis section gives instructions on how to install the packages needed, load library and activate the user interface.\n\n\nIntroduction\nThis section explains why we develop the TriplotGUI package: The research background, previous work of triplot and our aim.\n\n\nDetails\nThis sections gives detailed manual in analysis and visualization settings of TriplotGUI. We will not focus on the produced visualization output and interpretation here. Such content will be covered by the Tutorial and use case session.\n\n\nTutorial(simple)\nThis section provides a simple example of how to use the TriplotGUI package. The users can either use code or the user interface to achieve the similar purpose. The code tutorial suits for users that is at beginner R level. The user interface tutorial suits for user that has limited epidemiology knowledge and limited experience in dealing with high dimensional data.\nThe dataset used is CAMP_2\n\n\nTutorial(complex)\nThis section provides a simple example of how to use the TriplotGUI package. The users can either use code or the user interface to achieve the similar purpose. The code tutorial suits for users that is at intermediate or advanced R level. The user interface tutorial suits for user that has epidemiology knowledge and experience in dealing with high dimensional data.\nThe dataset used is HealthyNordicDiet_2.\n\n\nUse Case\nThis section shows a smoother example of how to use the interface to perform data analysis and interpretation, omiting the detailed explanation in the tutorial.\nThe dataset used is ExposomeChallenge.\n\n\nPaper\nThis sections covers relevant papers of our TriplotGUI package\n\n\nAbout us\nThis sections give brief introduction of the contributors to this package.\n\n\nContact us\nIf you have questions in the package, tutorial and this website, or would like to seek collaboration, please contact yingxiao@chalmers.se"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Install packages\nPlease run the following code to install the packages needed. This needs to be run in your R console.\n\nCode### I may need mix Omics and impute not sure\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"mixOmics\")\nBiocManager::install(\"impute\")\nBiocManager::install(\"preprocessCore\")\nBiocManager::install(\"GO.db\")\n\ninstall.packages(\"remotes\")\nremotes::install_gitlab(\"YingxiaoYan/TriplotGUI\")\n\n\nThings may pop up in your console and ask you for some choices, we provide some recommedations here. We are conservative in the choices to avoid any unforseen conflict and complications in packages.\n\n\nSometimes it pops up in your console to ask you which package you would like to update. We recommened you to chose 3.None\n\n\nSometimes it pops up in your console to ask Do you want to install from soutce packages which need compliation . We recommended you to chose no\n\n\nSometimes it pops up in your console to ask Update all/some/none?[a/s/n] . We recommended you to chose n\n\nLoad libraries\nRun the following code in your R console.\n\nCodelibrary(TriplotGUI)\n\n\nDownload the example data for user interface\nGo to my git repository. Click the download icon at the right side and download the Exampledata.rar. Unzip it and you will find the data that will be used as example in the tutorial.\nChange the maximize size of data file allowed\nThe default of our package has allows data of maximize size of 5 MB. To change that, you could specify the maximize file size you prefer using the code below. Run your code in the console. However, we do not recommend to use data with more than 5 MB in our tool. The bigger the dataset, the slower it will be. It might be beneficial for you to do some variable selection before putting the data in out tool.\n\nCodefilesize=100  ##MB\noptions(shiny.maxRequestSize=filesize*1024^2)\n\n\nActivate the user interface\nRun the following code in your R console.\n\nCodeTriplotGUI_shiny_tryout()"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Overview\n   Most observational epidemiology research studies associations between single exposure and outcome. Emerging exposure- and outcome-wide studies aim to more broadly identify potential risk factors and their health effects. Although omics technologies have permitted linking exposures to outcomes via molecular data, large-scale exploration of mediating mechanisms is frequently lacking. Likely because Omics data are often high dimensional and there is a lack of effective tools for direct interpretation of the complex relationships between multiple exposures, Omics and outcomes. We therefore developed the TriplotGUI tool to advance exposome-risk assessment and facilitate the visualization and interpretation of such complex associations via metabolic regulation.\nThe application adopted a highly reactive stepwise modular design, where one or two modules represents one of the 6 following steps and the change of input from an early step will feed into a later step.\n \n\n\nSteps\nStep1: Data reduction of omics data\nTransforms the Omics data into a number of components using principal component analysis (PCA) or weighted correlation network analysis (WGCNA).\nStep2A: Exposures’ correlations\nAssesses correlations between exposures and component scores from Step 1. Confounders can be adjusted.\nStep2B: Outcomes’ associations\nAssesses risk associations between outcomes and the component scores from Step 1. Confounders can be adjusted.\nStep3: Visualization of Triplot\nThe component scores and loadings, the correlation coefficients and the risk associations’ estimates as beta coefficients or odds ratio, are co-visualized as 3 different layers in one 2-dimensional plot.\nStep4: Mediation analysis and visualization\nPerforms mediation analysis on the selected exposures and outcomes, using component scores as mediators. The mediation estimates (i.e. direct, indirect and total effect) or the proportion mediated are then co-visualized with the correlation coefficients of the selected exposures and the risk associations’ estimates as beta coefficients or odds ratio of the selected outcomes as 3 different layers in one 2-dimensional plot.\nStep5: Compare correlations, associations and mediations\nThe correlations, associations, mediation estimates and proportion mediated as well as their significance levels can be visualized through heatmap.\nStep6: Download data\nThe intermediate data and the generated results can be viewed and downloaded. The object that saves all relevant information can be downloaded.\n We will give detailed explanations of statistical data analysis and visualization in each step in the workflow session."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Relevant papers",
    "section": "",
    "text": "Download main paper \n\n\n\n\nMain paper\n\n\n\nSchillemans T, Shi L, Liu X, Åkesson A, Landberg R, Brunius C. Visualization and Interpretation of Multivariate Associations with Disease Risk Markers and Disease Risk-The Triplot. Metabolites. 2019 Jul 6;9(7):133. doi: 10.3390/metabo9070133\n\n\n\n\n\n\nOther papers\n\nBodén S, Zheng R, Ribbenstedt A, Landberg R, Harlid S, Vidman L, Gunter MJ, Winkvist A, Johansson I, Van Guelpen B, Brunius C. Dietary patterns, untargeted metabolite profiles and their association with colorectal cancer risk. Sci Rep. 2024 Jan 26;14(1):2244. doi: 10.1038/s41598-023-50567-6\nSchillemans T, Yan Y, Ribbenstedt A, Donat-Vargas C, Lindh CH, Kiviranta H, Rantakokko P, Wolk A, Landberg R, Åkesson A, Brunius C. OMICs Signatures Linking Persistent Organic Pollutants to Cardiovascular Disease in the Swedish Mammography Cohort. Environ Sci Technol. 2024 Jan 16;58(2):1036-1047. doi: 10.1021/acs.est.3c06388.\nTitova OE, Brunius C, Warensjö Lemming E, Stattin K, Baron JA, Byberg L, Michaëlsson K, Larsson SC. Comprehensive analyses of circulating cardiometabolic proteins and objective measures of fat mass. Int J Obes (Lond). 2023 Nov;47(11):1043-1049. doi: 10.1038/s41366-023-01351-z"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a brief overview of projects that I significantly shaped throughout the entire project life cycle. In academic terms, this mostly corresponds to first-author publications (single and shared). If you’re interested in a full list of projects I have been involved in, please check out my CV."
  },
  {
    "objectID": "project.html#cmpe",
    "href": "project.html#cmpe",
    "title": "Projects",
    "section": "\n1 Consistency Model Posterior Estimation",
    "text": "1 Consistency Model Posterior Estimation\n\nPreprint (arXiv)\nConsistency models for neural posterior estimation (CMPE) is a new free-form conditional sampler for scalable, fast, and amortized simulation-based inference with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem."
  },
  {
    "objectID": "project.html#multinpe",
    "href": "project.html#multinpe",
    "title": "Projects",
    "section": "\n2 Deep Fusion for Multimodal Simulation-Based Inference",
    "text": "2 Deep Fusion for Multimodal Simulation-Based Inference\n\nPreprint (arXiv)\nWe present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy and better performance under partially missing data."
  },
  {
    "objectID": "project.html#data-efficient-amortized-bayesian-inference-via-self-consistency-losses",
    "href": "project.html#data-efficient-amortized-bayesian-inference-via-self-consistency-losses",
    "title": "Projects",
    "section": "\n3 Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses",
    "text": "3 Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses\n\nShort Paper (NeurIPS UniReps 2023)\nWe propose a method to improve the efficiency and accuracy of amortized Bayesian inference by leveraging universal symmetries in the probabilistic joint model \\(p(\\theta,y)\\). In a nutshell, we invert Bayes’ theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators."
  },
  {
    "objectID": "project.html#meta-uncertainty-BMC",
    "href": "project.html#meta-uncertainty-BMC",
    "title": "Projects",
    "section": "\n4 Meta-Uncertainty in Bayesian Model Comparison",
    "text": "4 Meta-Uncertainty in Bayesian Model Comparison\n\nPaper (AISTATS 2023) | Code | ’Project website | Poster | Presentation (15min)\nMeta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies."
  },
  {
    "objectID": "project.html#bayesflow-amortized-bayesian-workflows-with-neural-networks",
    "href": "project.html#bayesflow-amortized-bayesian-workflows-with-neural-networks",
    "title": "Projects",
    "section": "\n5 BayesFlow: Amortized Bayesian Workflows With Neural Networks",
    "text": "5 BayesFlow: Amortized Bayesian Workflows With Neural Networks\n\nSoftware Paper | Documentation | BayesFlow Forums (new!)\nBayesFlow is a Python library for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized."
  },
  {
    "objectID": "project.html#jana-jointly-amortized-neural-approximation-of-complex-bayesian-models",
    "href": "project.html#jana-jointly-amortized-neural-approximation-of-complex-bayesian-models",
    "title": "Projects",
    "section": "\n6 JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models",
    "text": "6 JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models\n\nPaper (UAI 2023) | Python library\nJANA is a new amortized solution for intractable likelihood functions and posterior densities in Bayesian modeling. It trains three networks to learn both an approximate posterior and a surrogate model for the likelihood, enabling amortized marginal likelihood and posterior predictive estimation."
  },
  {
    "objectID": "project.html#sbi-model-misspecification",
    "href": "project.html#sbi-model-misspecification",
    "title": "Projects",
    "section": "\n7 Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks",
    "text": "7 Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks\n\nPaper (GCPR 2023, Best Paper Honorable Mention) | Code | Poster Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic domain shifts during inference.\n\nCode#don't run code, but show code\n3\n\n\n\nCode#run code, and show code, don't show output\n4\n\n\n\n\n[1] 2\n\n\n[1] 3\n\n\n\nCode#run code, and show code, and show output\n5\n\n[1] 5\n\nCodes&lt;-function(x){x+1}\ns(23)\n\n[1] 24\n\n\nFOR REFERENCE: Graduation Cap\n # stopped working 5/1/2023?"
  },
  {
    "objectID": "pub_ho.html",
    "href": "pub_ho.html",
    "title": "Publication & honor",
    "section": "",
    "text": "Luyan Wu, Wenhui Zeng, Liandong Feng, Yuxuan Hu, Yidan Sun, Yingxiao Yan, HongYuan Chen & Deju Ye. An activatable radiometric near-inflared fluorescent probe for hydrogen sulfide imaging in vivo. Science China Chemistry. 2020;63(5):741-750 Yan Y, Smith E, Melander O, Ottosson F. The association between plasma metabolites and future risk of all‐cause mortality. Journal of Internal Medicine. 2022;292(5):804-815.  Schillemans T, Yan Y, Ribbenstedt A, Donat-Vargas C, Lindh C, Kivirante H, Wolk A, et al.OMICs signatures linking persistent organic pollutants to cardiovascular disease in the Swedish Mammography Cohort [Submitted]"
  },
  {
    "objectID": "pub_ho.html#publications",
    "href": "pub_ho.html#publications",
    "title": "Publication & honor",
    "section": "",
    "text": "Luyan Wu, Wenhui Zeng, Liandong Feng, Yuxuan Hu, Yidan Sun, Yingxiao Yan, HongYuan Chen & Deju Ye. An activatable radiometric near-inflared fluorescent probe for hydrogen sulfide imaging in vivo. Science China Chemistry. 2020;63(5):741-750 Yan Y, Smith E, Melander O, Ottosson F. The association between plasma metabolites and future risk of all‐cause mortality. Journal of Internal Medicine. 2022;292(5):804-815.  Schillemans T, Yan Y, Ribbenstedt A, Donat-Vargas C, Lindh C, Kivirante H, Wolk A, et al.OMICs signatures linking persistent organic pollutants to cardiovascular disease in the Swedish Mammography Cohort [Submitted]"
  },
  {
    "objectID": "pub_ho.html#conference-and-awards",
    "href": "pub_ho.html#conference-and-awards",
    "title": "Publication & honor",
    "section": "\n2 Conference and awards",
    "text": "2 Conference and awards\n\n\nMetabolomics 2022, Valencia, Spain. - Speaker, with award\n\n\nSwedish Bioinformatics workshop 2022,Umeå, Sweden. - Speaker\n\n\nNordic rye forum 2022, Stockholm, Sweden - Speaker"
  },
  {
    "objectID": "pub_ho.html#key-competence",
    "href": "pub_ho.html#key-competence",
    "title": "Publication & honor",
    "section": "\n3 Key competence",
    "text": "3 Key competence\n\n\nR/shiny (data cleaning, manipulation, analysis, and visualization) - Expert\n\n\nBiostatistics and epidemiological research - Expert\n\n\nSPSS, Excel & Microsoft Office suite - Expert\n\n\npython - Intermediate"
  },
  {
    "objectID": "pub_ho.html#languages",
    "href": "pub_ho.html#languages",
    "title": "Publication & honor",
    "section": "\n4 Languages",
    "text": "4 Languages\n\n\nMandarin (Native speaker)\n\n\nEnglish (Native level)\n\n\nSwedish (Intermediate/B1 level, SAS certificated)"
  },
  {
    "objectID": "pub_ho.html#references",
    "href": "pub_ho.html#references",
    "title": "Publication & honor",
    "section": "\n5 References",
    "text": "5 References\nFilip Ottoson Postdoctoral Researcher – Lund University Email: filip.ottoson@med.lu.se Tel: +46739699000 Carl Brunius Associate Professor – Chalmers University of Technology Email: calbrunius@chalmers.se Tel:\n\n© 2023 Yingxiao Yan ★★★★★. All rights reserved"
  },
  {
    "objectID": "pub_ho.html#news",
    "href": "pub_ho.html#news",
    "title": "Publication & honor",
    "section": "\n6 News",
    "text": "6 News\n\nJanuary 23, 2024: I will be at Bayes on the Beach 2024 in February. I will give a talk about reliable amortized Bayesian inference with neural networks and co-lead a workshop on amortized Bayesian workflows.\nDecember 2023: We launched the BayesFlow Forums! The BayesFlow Forums provide a community for asking and answering questions about all aspects of BayesFlow and amortized Bayesian workflows in general. Hop over and join the discussion!\nOctober 11, 2023: In our recent preprint, we make simulation-based inference more data-efficient by leveraging self-consistency properties of the Bayesian joint model. You find the preprint on arXiv. Update: Published in the NeurIPS UniReps workshop.\nSeptember 22, 2023: Our paper “Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks” has been awarded the DAGM GCPR Honorable Mention at this year’s German Conference on Pattern Recognition. Huge thanks to my great co-authors!\nSeptember 6, 2023: Our paper “Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks” has been accepted with oral presentation at the German Conference for Pattern Recognition 2023. Learn more at the project website.\nJune 21, 2023: I will be at the ELLIS Doctoral Symposium 2023 in Helsinki from August 28 to September 1, 2023. Absolutely thrilled to meet other PhD students and researchers and talk about machine learning research in beautiful Helsinki!\nMay 9, 2023: Our paper “JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models” has been accepted to UAI 2023 (Uncertainty in AI). Check out the project page to find out more!\nMarch 29, 2023: I’ll be presenting our paper on Meta-Uncertainty in Bayesian Model Comparison at AISTATS 2023 from April 25-27 in Valencia, Spain. You find links to the paper and code on my Projects page or on the dedicated paper website. If you see me around at AISTATS, let’s chat!"
  },
  {
    "objectID": "pub_ho.html#featured-blog-posts",
    "href": "pub_ho.html#featured-blog-posts",
    "title": "Publication & honor",
    "section": "\n7 Featured Blog Posts",
    "text": "7 Featured Blog Posts\n\n\n\n\nCode#don't run code, but show code\n3\n\n\n\nCode#run code, and show code, don't show output\n4\n\n\n\n\n[1] 2\n\n\n\nCode#run code, and show code, and show output\n5\n\n[1] 5"
  },
  {
    "objectID": "simple.html",
    "href": "simple.html",
    "title": "Tutorial(simple) - User Interface",
    "section": "",
    "text": "Before starting, please make sure you have installed the TriplotGUI package following Setup."
  },
  {
    "objectID": "simple.html#upload-data",
    "href": "simple.html#upload-data",
    "title": "Tutorial(simple) - User Interface",
    "section": "Upload data",
    "text": "Upload data\nFirst upload"
  },
  {
    "objectID": "simple_code.html",
    "href": "simple_code.html",
    "title": "Tutorial(simple) - Code",
    "section": "",
    "text": "Before starting,please make sure you have installed the TriplotGUI package following Setup.\nThe code of at this page can be downloaded here"
  },
  {
    "objectID": "simple_code.html#check-camp_2",
    "href": "simple_code.html#check-camp_2",
    "title": "Tutorial(simple) - Code",
    "section": "\n3.1 Check CAMP_2",
    "text": "3.1 Check CAMP_2\nCheck CAMP_2 as a list:\n\nCodeclass(CAMP_2)\n\n[1] \"list\"\n\nCodenames(CAMP_2)\n\n[1] \"FoodData\"       \"MetaboliteData\" \"ClinData\""
  },
  {
    "objectID": "simple_code.html#check-datasets",
    "href": "simple_code.html#check-datasets",
    "title": "Tutorial(simple) - Code",
    "section": "\n3.2 Check datasets",
    "text": "3.2 Check datasets\nCheck the names of variables in each data:\n\nCodecolnames(CAMP_2$ClinData)\n\n [1] \"AGE\"               \"SEX\"               \"BMI\"              \n [4] \"triglycerides\"     \"total_cholesterol\" \"HDL\"              \n [7] \"LDL\"               \"GGT\"               \"ALT\"              \n[10] \"AST\"               \"creatinine\"        \"Urea_nitrogen\"    \n[13] \"Uric_acid\"         \"Fasting_glucose\"  \n\nCodecolnames(CAMP_2$MetaboliteData)\n\n [1] \"PC_P_20_0_22_6_\"                                                                \n [2] \"X1__3_4_Dihydroxyphenyl__7__4_hydroxy_3_methoxyphenyl__1_6_heptadiene_3_5_dione\"\n [3] \"Unknown_675.6485_9.35\"                                                          \n [4] \"Unknown_1066.9034_10.16\"                                                        \n [5] \"PC_42_8_\"                                                                       \n [6] \"Unknown_931.7610_9.68\"                                                          \n [7] \"TG_52_0_\"                                                                       \n [8] \"CE_22_4_\"                                                                       \n [9] \"Neutral_glycosphingolipids1023.67\"                                              \n[10] \"TG_58_10_\"                                                                      \n[11] \"Neutral_glycosphingolipids971.72\"                                               \n[12] \"Unknown_914.7433_10.06\"                                                         \n[13] \"Cucurbic_acid\"                                                                  \n[14] \"DG_16_0_18_2_\"                                                                  \n[15] \"Indoleacrylic_acid\"                                                             \n[16] \"PC_O_20_0_22_6_\"                                                                \n[17] \"Unknown_948.6589_8.51\"                                                          \n[18] \"Unknown_858.5318_5.49\"                                                          \n[19] \"O_propanoyl_carnitine\"                                                          \n[20] \"DG_38_5_\"                                                                       \n\nCodecolnames(CAMP_2$FoodData)\n\n [1] \"Refined_grains\" \"Coarse_grains\"  \"Red_meat\"       \"Poutry\"        \n [5] \"Seafood\"        \"Egg\"            \"Animal_organs\"  \"Vegetables\"    \n [9] \"Fruits\"         \"Potatos\"        \"Legumes\""
  },
  {
    "objectID": "simple_code.html#check-variables-class",
    "href": "simple_code.html#check-variables-class",
    "title": "Tutorial(simple) - Code",
    "section": "\n3.3 Check variables’ class",
    "text": "3.3 Check variables’ class\nWe Transform the data to dataframe format and then use TriplotGUI’s checkdata() function in to examine the class of variables.\n\nCodeClinData&lt;-as.data.frame(CAMP_2$ClinData)\nMetaboliteData&lt;-as.data.frame(CAMP_2$MetaboliteData)\nFoodData&lt;-as.data.frame(CAMP_2$FoodData)\n\nClinData_check&lt;-checkdata(ClinData)\nMetaboliteData_check&lt;-checkdata(MetaboliteData)\nFoodData_check&lt;-checkdata(FoodData)\n\n\n\nCodeClinData_check$class_sumamry_statistics\n\n$check_class_vector\n              AGE               SEX               BMI     triglycerides \n        \"numeric\"          \"factor\"         \"numeric\"         \"numeric\" \ntotal_cholesterol               HDL               LDL               GGT \n        \"numeric\"         \"numeric\"         \"numeric\"         \"numeric\" \n              ALT               AST        creatinine     Urea_nitrogen \n        \"numeric\"         \"numeric\"         \"numeric\"         \"numeric\" \n        Uric_acid   Fasting_glucose \n        \"numeric\"         \"numeric\" \n\n$check_class_table\ncheck_class_vector\n factor numeric \n      1      13 \n\nCodeMetaboliteData_check$class_sumamry_statistics$check_class_table\n\ncheck_class_vector\nnumeric \n     20 \n\nCodeFoodData_check$class_sumamry_statistics\n\n$check_class_vector\nRefined_grains  Coarse_grains       Red_meat         Poutry        Seafood \n     \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\" \n           Egg  Animal_organs     Vegetables         Fruits        Potatos \n     \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\"      \"numeric\" \n       Legumes \n     \"numeric\" \n\n$check_class_table\ncheck_class_vector\nnumeric \n     11"
  },
  {
    "objectID": "simple_code.html#check-sanities-for-variables",
    "href": "simple_code.html#check-sanities-for-variables",
    "title": "Tutorial(simple) - Code",
    "section": "\n3.4 Check sanities for variables",
    "text": "3.4 Check sanities for variables\nWhether each variable contians missing (NA) or abnormal values (e.g. NAN, Inf, blank value) can also be checked\n\nCodeClinData_check$everycolumn\n\n\n\n  \n\n\nCodeMetaboliteData_check$everycolumn\n\n\n\n  \n\n\nCodeFoodData_check$everycolumn\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou shall only continue when the class of varaibles are correct and the missing or abnormal values in the variable are properly handled."
  },
  {
    "objectID": "simple_code.html#build-data-for-analysis",
    "href": "simple_code.html#build-data-for-analysis",
    "title": "Tutorial(simple) - Code",
    "section": "\n3.5 Build data for analysis",
    "text": "3.5 Build data for analysis\nWe see food data as our exposures and BMI, triglycerides, total_cholesterol, HDL and LDL as outcomes. We want to explore their relationships through the metabolomics data, using metabolites as assumed mediators. Sex and age are used as potential confounders for exposure.-mediator and mediator-outcome association.\n\nCodeexposure1&lt;-FoodData\nOmics1&lt;-MetaboliteData\noutcome1&lt;-ClinData[,c(\"BMI\",\"triglycerides\",\"total_cholesterol\",\"HDL\",\"LDL\")]\ncovariates1&lt;-ClinData[,c(\"SEX\" ,\"AGE\")]"
  },
  {
    "objectID": "simple_code.html#step-1",
    "href": "simple_code.html#step-1",
    "title": "Tutorial(simple) - Code",
    "section": "\n4.1 Step 1",
    "text": "4.1 Step 1\nUsing TriplotGUI package, first we perform dimension reduction, i.e. principal component analysis (PCA) on metabolomics data.\n\nCodereduced_Omics1&lt;-PCAorWGCNA_plots(dataframe=Omics1,\n                                 pc_num = 5,\n                                 option=\"PCA\")\n\n\nYou can see scree plot, score plot, loadings plot and biplot at this stage.\n\nCodereduced_Omics1$scree_plot\n\nplot\n\n\n\nCodereduced_Omics1$score_plot\n\nplot\n\n\n\nCodereduced_Omics1$loading_plot\n\nplot\n\n\n\nCodereduced_Omics1$scoreloading_plot\n\nplot\n\n\n\n\n\nWe then build a TPObject, which is used for saving information and pass them through steps in TriplotGUI.\n\nCodescores=reduced_Omics1$object$scores\nloadings&lt;-reduced_Omics1$object$loadings\nvariance&lt;- reduced_Omics1$object$variance\nTPObject1&lt;-makeTPO(scores=scores,\n                   loadings=loadings,\n                   variance=variance)\n\n\nMaking TriPlotObject (TPO)\n--------------------------\nLoading matrix has 20 variables and 20 components.\n\nScore matrix has 300 observations and 20 components.\n\nTPO has 0 attached correlations.\nTPO has 0 attached risks."
  },
  {
    "objectID": "simple_code.html#step-2",
    "href": "simple_code.html#step-2",
    "title": "Tutorial(simple) - Code",
    "section": "\n4.2 Step 2",
    "text": "4.2 Step 2\nThe correlations between principal component (PC) scores and food items were calculated using Pearson correlations, adjusting for confounders. The associations between PC scores and risk markers were investigated using linear regression.\nCorrelation matrix of correlation coefficients and p values between the TPO scores saved in the TPObject and the dietarry variables in the data is generated, using a pair-wise Pearson correlations, adjusting for confounders\n\nCodeCorrelations_object&lt;-makeCorr(TPObject=TPObject1,\n                              corrData=exposure1,\n                              confounder=covariates1,\n                              method=\"pearson\")\n\n\nThe result of correlations are then added into the TPObject.\n\nCodeTPObject2&lt;-addCorr(TPObject=TPObject1,\n                   Corr=Correlations_object$cor_estimate,\n                   Corr_p=Correlations_object$cor_pvalue)\n\n\nAdding correlation to TPO\n-------------------------\nPlease ensure same number of components in TPO and correlation matrix\n\nTPO has 11 attached correlations.\nTPO has 0 attached risks.\n\n\nMatrices of risk estimates and p values between the TPO scores saved in the TPObject and the outcome variables are generated, using linear regression, adjusting for confounders.\n\nCodeRisks_object&lt;-coefficient_get(TPObject=TPObject2,\n                              outcome=outcome1,\n                              confounder=covariates1)\n\n\nThe result of risk estimates are then added into the TPObject.\n\nCodeTPObject3&lt;-addRisk(TPObject=TPObject2,\n                   Risk=Risks_object)\n\n\nAdding risk to TPO\n------------------\nPlease add risks one by one with component in rows and estimates, se:s and (optionally) p-values in columns\nPlease ensure same number of components in TPO and risk matrix\n\nTPO has 11 attached correlations.\nTPO has 5 attached risks."
  },
  {
    "objectID": "simple_code.html#step-3",
    "href": "simple_code.html#step-3",
    "title": "Tutorial(simple) - Code",
    "section": "\n4.3 Step 3",
    "text": "4.3 Step 3\nGenerate Triplot: Note that Triplot can be generated from any TPObject.\n\nCodeTriplot_object3&lt;-TriplotGUI(TPObject3,\n                            riskOR=F) #risks is shown as coeffcients, not odds ratio\n\n\nPlotting the triplot\n\nCodeTriplot_object3$triplot\n\nplot\n\n\n\n\n\nIn the triplot, PC1 reflected metabolites that were positively associated with BMI and also correlated with a high intake of meat and refined grains, adjusting for age and sex. This implies that the metabolite features that contributes most to PC1, are likely to provide useful information for a potential mechanism of how red meat and refrained grains intake may affect BMI."
  },
  {
    "objectID": "simple_code.html#step-4",
    "href": "simple_code.html#step-4",
    "title": "Tutorial(simple) - Code",
    "section": "\n4.4 Step 4",
    "text": "4.4 Step 4\nMediation analysis is then performed using the “product” method on the our exposures (i.e. Refined grains, Red meat), mediator (i.e. PC1) and outcome (i.e. BMI) of interest, adjusting for age and sex for both exposure-mediator and mediator-outcome relationship.\n\nCodemediation_object3&lt;-\n  get_mediation_traditional (mediator=TPObject3$scores[,c(1:2),drop=F], \n                             # Specfiying at least 2 components so that there can be a 2-dimensional plot\n                             exposure=exposure1[,c(\"Refined_grains\",\"Red_meat\"),drop=F],\n                             outcome=outcome1[,\"BMI\",drop=F],\n                             confounder_ME=covariates1,\n                             confounder_OE=covariates1)\n\n\nLook at the barplot to have a more direct view on direct, indirect and total effects\n\nCodemediation_plot_object3&lt;-plot_mediation(mediation_object3,\n                                       by_row=\"one_by_one\")\n\n\n\nCodemediation_plot_object3\n\n$Refined_grains_PC1_BMI\n\n\nplot\n\n\n\n\n\n$Red_meat_PC1_BMI\n\n\nplot\n\n\n\n\n\n$Refined_grains_PC2_BMI\n\n\nplot\n\n\n\n\n\n$Red_meat_PC2_BMI\n\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe barplot showed direct, indirect and total effect for each exposure-mediator-outcome combination. It is a convenient tool to check the direction and magnitude of mediation estimates. Red color represents significant positive effect (p&lt;0.05); Blue color represents significant negative effect (p&lt;0.05). Grey represents insignificant effect. One star for p&lt;0.05; Two stars for p&lt;0.01; Three stars for p&lt;0.001;\n\n\nAdd the mediation result into the TPObject.\n\nCodeTPObject4&lt;-add_mediation(TPObject3, mediation_object3)\n\n\nAdding mediation to TPO\n-------------------------\nNote that the exposures, mediators and outcomes are selected and not necessarily reflect total number of variables\n\nTPO mediation has used 2 mediators, 2 exposures, 1 outcomes\nMediators are PC1, PC2 \nExposures are Refined_grains, Red_meat \nOutcomes are BMI\n\n\nMake the triplot with mediation\n\nCodeTriplot_object4&lt;-mediation_triplot(TPObject4,\n                                   which_show = \"IE\")\n\n\nOnly showing the mediaiton estimates\n\nCodeTriplot_object4$mediation_plot\n\nplot\n\n\n\n\n\nPlotting the triplot with mediation\n\nCodeTriplot_object4$final_plot\n\nplot\n\n\n\n\n\nBase on the barplot and the triplot with mediaiton, we observed a significant mediation effect through PC1 for Red_meat-BMI and refined_grain-BMI association. This further implies that the metabolite features that contributes to PC1 are likely to mediate the pathway from red meat and refined grain intake to BMI change."
  },
  {
    "objectID": "simple_code.html#step-5",
    "href": "simple_code.html#step-5",
    "title": "Tutorial(simple) - Code",
    "section": "\n4.5 Step 5",
    "text": "4.5 Step 5\nUsers can check the heatmaps of correlations, risk estimations and mediation results from the TPObject through the checkTPO_ggplot() function in TriplotGUI\n\nCodecheckTPObject4&lt;-checkTPO_ggplot(TPObject4)\n\n\n\nCodecheckTPObject4$corr_coefficients\n\nplot\n\n\n\nCodecheckTPObject4$risk_coefficients\n\nplot\n\n\n\nCodecheckTPObject4$med_coefficients\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nOne star for p&lt;0.05; Two stars for p&lt;0.01; Three stars for p&lt;0.001.\nNote that in the plot of checkTPbject4$med_coefficients, only the slected exposures, mediators and outcomes that we use to do mediation on will show up. The rows in the heatmaps are mediators and the column represents exposure-outcome pairs. For each exposure_outcome pair, 5 result are shown: IE (indirect effect), DE (direct effect), TE (total effect), IE/(IE+DE) and IE/(abs(IE)+abs(DE))×(IE+DE)/(abs(IE+DE)). Please see here for more information. The “T” before the names of exposures-outcome pairs means that this mediation is using the “Traditional” product method."
  },
  {
    "objectID": "simple_code.html#step-6",
    "href": "simple_code.html#step-6",
    "title": "Tutorial(simple) - Code",
    "section": "\n4.6 Step 6",
    "text": "4.6 Step 6\nWe can save all your output, including data, results and visualization output through the save() function as an rda file.\n\nCodesave(exposure1,Omics1,outcome1,covariates1,\n     reduced_Omics1,Correlations_object,Risks_objects,\n     mediation_object3,mediation_plot_object3,\n     TPObject1,TPObject2,TPObject3,TPObject4,\n     checkTPObject4,\n     \"Tutorial_simple_output.rda\")\n\n\nFOR REFERENCE: Graduation Cap\n # stopped working 5/1/2023?"
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "We will go through the statistical analysis behind TriplotGUI and also explain the details of out data analysis and visulization in each step. If you want to search the meaning of something specific (e.g. a button, a box), please press ctrl+F and enter the keyword, you will probably find what you need.\nIf you have not open the app yet, follow what is written in the Setup and open the interface with TriplotGUI_shiny()"
  },
  {
    "objectID": "workflow.html#data-for-pcawgcna",
    "href": "workflow.html#data-for-pcawgcna",
    "title": "Workflow",
    "section": "\n1.1 Data for PCA/WGCNA",
    "text": "1.1 Data for PCA/WGCNA\n\n1.1.1 Basics\nThe darkblue box is where you put in the Omics data. The data should meet the following criteria.\n\nThe data should be in either csv, xlsx, rds, or rda format. The data in rds format should be a dataframe. If the data is in rda format, the rda object should only contain the data as a dataframe and be in the same directory as your working directory (You could check this by getwd() in the R console)\nThe rows of data should represent observations and the columns of data should represents variables(omics features).\nThe names of variables (column names) should not contain special characters including ' ', '(', ')', ':', '/', '-', ',', '@'.\nNo missing values are allowed in the data\nThe number and order of observations should be same across all the uploaded data\n\n\n\n\n\n\n\nNote\n\n\n\nWe do not provide the functionality of handling missing data or other data pre-processing steps (e.g. transformation, normalization) in our application due to the variability of possible methods.\n\n\n\n1.1.2 Emerged\nAfter you upload the data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Force all variables to numeric.\n\nInspect variable class This is a button that help you inspect if the variables are in correct format, since you may not want your metabolite features in character format when you want to use their numeric values. Click the button and explore.\n\nremove variables: This button allows you to remove variables from your uploaded data. Click the button, remove some variables and click OK. What variables are removed are then printed out and they will not be included in the Omics data to enter the downstream analysis. We also make this removal reversible so that every time when you reclick this button, the data will go back to the state where no variables are removed.\nWe add this button so that users will not need to manually remove redundant variables that will actually not be used in data analysis.\n\nForce all variables to numeric: Upon clicking, this button simply forces all the variables to numeric. Since we want to perform PCA or WGCNA on this Omics data, we want all Omics variables to be in numeric format\n\n1.1.3 And more\nAt the right side of the upload data, there is a button called Reset everything. This where you can click remove all your uploads and settings and make the interface go back to a default state."
  },
  {
    "objectID": "workflow.html#auxilary_data",
    "href": "workflow.html#auxilary_data",
    "title": "Workflow",
    "section": "\n1.2 Auxilary data",
    "text": "1.2 Auxilary data\nThe lightblue box is where you put in the auxilary data. Currently, the variables in the data uploaded here can be used for 2 things (1) Customizing the color, size and shape of scores. See section 1.4.2. (2) Providing pairing information for outcomes’ associations. See section 1.1.1.\n\n1.2.1 Basics\nThe requirement for input data is the same as 1.1.1.\n\n1.2.2 Emerged\nAfter you upload the axuliary data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\n\nChange variable class: This button provide your choices in transforming variables to either factor and numeric variables.\nWe add this button to faciliate user in tranforming variables to numeric and factor format, corresponding to numeric and caregorical variables."
  },
  {
    "objectID": "workflow.html#data-analysis-settings",
    "href": "workflow.html#data-analysis-settings",
    "title": "Workflow",
    "section": "\n1.3 Data analysis settings",
    "text": "1.3 Data analysis settings\nThe red box is where you make choices in how do you want your data analysis to be performed\n\n1.3.1 Basics\n\nMethod: Users choose to either use principal component analysis (PCA) or weighted correlation network analysis (WGCNA) to perform data reduction. The default is PCA\nPCA function: Users choose what PCA function, prcomp or principal, will be used for principal component analysis. The default is prcomp\n\nMax number of components: This button shows up when the Method is PCA. It decides how many principal component will be used in the downstream correlation& association analysis. The default is set as around the half of the total number of omics variables used for analysis.\nMinimum number for variables per module: This button shows up when the Method is WGCNA. It decides the minimum number of variables that is allowed in each cluster of WGCNA. The default number is 2.\n\nCenter: If the omics variables will be zero centered before the analysis take place. The default is Yes.\nScale: If the omics variables will be scaled to have unit variance before the analysis take place. The default is Yes.\n\n1.3.2 Emerged\n\n\nrotate: The button will show up and When PCA function is chosen as principal. How users would like to transform principal components. Please check here for the explanation fo rotate. The default is set as none in TriplotGUI.\n\n\n\n\n\n\n\nNote\n\n\n\nYou maybe heard about PCA but not WGCNA. In brief, WGCNA is to separate omics variables into clusters, perform PCA on each cluster and extract the first principal component of each cluster as the principal components of WGCNA. The clustering method can be different. In TriplotGUI, we use the default hierarchical clustering method provided by cutreeDynamic() in the dunamicTreeCut package"
  },
  {
    "objectID": "workflow.html#visualization-settings",
    "href": "workflow.html#visualization-settings",
    "title": "Workflow",
    "section": "\n1.4 Visualization settings",
    "text": "1.4 Visualization settings\nThe orange box is where users can modify their visulization outputs\n\n1.4.1 Basics\n\nWhich plots: Four options are provided: (1) scores: The scores of principal components are plotted. (2)loadings: The loadings of principal components are plotted. (3) biplots: Both scores and loadings are plotted on the same plot. (4) screeplot: How much variance does each principal component contribute to is plotted. In default mode, none of the 4 options are selected\nComponent/module on first axis: The principal component that will be plotted on the x-axis. The default is 1 (the first one).\nComponent/module on second axis: The principal component that will be plotted on the y-axis. The default is 2 (the second one).\nWhich type of loading to use Two options are provided Eigenvectors and Eigenvectors × squareroot(eigenvalues). The 2 calulations, respectively, correspond to what is referred to as “rotation” in the prcomp package and “Loadings” in the principal package\n\n1.4.2 Emerged\n\nLoading labels: Upon clicking loadings or biplot in Which plots. The button will show up. It decides if labels of loadings will be shown on the plot. The default is Yes\nScale biplot: Upon clicking biplot in Which plots. The button will show up. It decides if scores and loadings will be both scaled to a visible level by adjusting the limit of axises. The default is Yes\nCut ratio or absolute value: Upon clicking loadings or biplot in Which plots. The button will show up. It is useful when the users only want to show loadings bigger than a certain value or bigger than a certain percent of the maximum loading. The default is Absolute value\nLoadings cut absolute value: When selecting Absolute value in Cut ratio or absolute value, a slider will show up to let user decide the loadings below what value will not show up on the plot\nLoadings cut ratio: When selecting Cut ratio in Cut ratio or absolute value, a slider will show up to let user decide the loadings below how much percent of the maximum loading will not show up on the plot\nScore color by: Upon uploading the Auxilary data and clicking loadings or biplot in Which plots, this button will show up. Users can use one numeric or categorical variable in the auxilary data to color the scores points. By default, no variables are selected.\nScore shape by: Upon uploading the Auxilary data and clicking loadings or biplot in Which plots, this button will show up. Users can use one categorical variable in the auxilary data to change the shape of scores points. By default, no variables are selected.\nScore size by: Upon uploading the Auxilary data and clicking loadings or biplot in Which plots, this button will show up. Users can use one numeric variable in the auxilary data to change the size of score points. By default, no variables are selected."
  },
  {
    "objectID": "workflow.html#visualizations",
    "href": "workflow.html#visualizations",
    "title": "Workflow",
    "section": "\n1.5 Visualizations",
    "text": "1.5 Visualizations\nThe green box represents visualization outputs. With omics data uploaded, upon selecting Which plots in the data visualization setting, the corresponding plot are shown."
  },
  {
    "objectID": "workflow.html#exposure-data",
    "href": "workflow.html#exposure-data",
    "title": "Workflow",
    "section": "\n2.1 Exposure data",
    "text": "2.1 Exposure data\n\n2.1.1 Basics\nThe dark blue box is where you put in the exposure data. The requirement for exposure data is the same as 1.1.1.\n\n2.1.2 Emerged\nAfter you upload the exposure data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#Covaraite_data1",
    "href": "workflow.html#Covaraite_data1",
    "title": "Workflow",
    "section": "\n2.2 Covariate data",
    "text": "2.2 Covariate data\nThe light blue box is where you put in the covariate data. After removing redundant variables, the remaining covariate data can be used as confounders for the partial correlation between exposures and principal components\n\n2.2.1 Basics\nThe requirement for exposure data is the same as 1.1.1.\n\n2.2.2 Emerged\nAfter you upload the covariate data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#data-analysis-settings-1",
    "href": "workflow.html#data-analysis-settings-1",
    "title": "Workflow",
    "section": "\n2.3 Data analysis settings",
    "text": "2.3 Data analysis settings\nThe red box is where you make choices in how do you want your correlation analysis to be performed\n\n2.3.1 Basics\n\nMethod: Three methods inherited from the cor package are supported for continuous exposures: pearson, spearman, kendall. Please check cor package for more information. The default for TriplotGUI is pearson.\nManage missing values: The approaches for managing missing values inherited from the cor package are supported for continuous exposures. Please check cor package for more information. The default for TriplotGUI is everything.\nCategorical variables: Two options are provided Perform one-hot-encoding and Use original. When choosing Perform one-hot-encoding, categorical variables with n classes(n&gt;2) are transformed to n binary variables with 0 and 1 values and then used as numeric variables. When chosing Use original, no transformation is made on the categorical variabls and linear models are used for categorical exposure-principal component correlation analysis. The default is Use original.\n\n2.3.2 Emerged\n\n\nFull or partial correaltion Upon uploading the covariate data, the button will show up. Two options are provided: Full and Partial. Full means no covariates are used as confounders in the correlation analysis and Partial means that partial correlations, adjusting the same group of confounders, are performed on all exposures-principal components correlations."
  },
  {
    "objectID": "workflow.html#visualizations-1",
    "href": "workflow.html#visualizations-1",
    "title": "Workflow",
    "section": "\n2.4 Visualizations",
    "text": "2.4 Visualizations\nThe green box represents visualization outputs. With omics data from step 1 and exposure data uploaded, upon clicking the Refresh plot button, the correlations between exposures and the 2 selected components (selected from step 1) are shown. The figure can be downloaded by clicking Download figure."
  },
  {
    "objectID": "workflow.html#outcome-data",
    "href": "workflow.html#outcome-data",
    "title": "Workflow",
    "section": "\n3.1 Outcome data",
    "text": "3.1 Outcome data\n\n3.1.1 Basics\nThe dark blue box is where you put in the outcome data. The requirement for exposure data is the same as 1.1.1.\n\n3.1.2 Emerged\nAfter you upload the exposure data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#Covaraite_data2",
    "href": "workflow.html#Covaraite_data2",
    "title": "Workflow",
    "section": "\n3.2 Covariate data",
    "text": "3.2 Covariate data\nThe light blue box is where you put in the covariate data. After removing redundant variables, the remaining covariate data can be used as confounders for the partial correlation between outcomes and principal components\n\n3.2.1 Basics\nThe requirement for exposure data is the same as 1.1.1.\n\n3.2.2 Emerged\nAfter you upload the covariate data, A text will show up to tell you how many rows and columns do you have in the data frame. Three following buttons also emerge: Inspect variable class, remove variables, Change variable class.\n\nInspect variable class Same as 1.1.2\nremove variables: Same as 1.1.2\nChange variable class: Same as 1.2.2"
  },
  {
    "objectID": "workflow.html#data-analysis-settings-2",
    "href": "workflow.html#data-analysis-settings-2",
    "title": "Workflow",
    "section": "\n3.3 Data analysis settings",
    "text": "3.3 Data analysis settings\nThe red box is where you make choices in how do you want your risk association analysis to be performed.\n\n\n\n\n\n\nNote\n\n\n\nFor outcomes - principal-components associations, statistical analysis are performed differently based on different classes of outcome and information provided. The table below summarized the methods and R packages used.\n\n\n\n\n3.3.1 Basics\n\n\nlog(risk estimates): Users can choose whether they want their risk estimations to be presented as beta coefficients or odds ratios. Two options are provided Yes (useful for odds ratio) and No (useful for beta coefficient). The default is No (useful for beta coefficient)\n\n\n3.3.2 Emerged\n\nDo confounder adjustment: Upon uploading the covariate data, the button will show up. The users can choose if confounder will be adjusted for all outcomes-principal components associations, using the same group of confounders. The default is No.\nMultinomial: If categorical variables with n&gt;2 class is uploaded or created from outcome variables, the button will show up. By selecting No, one-hot-encoding will be performed on these categorical variables to transform them to n binary variables with 0 and 1 values. By selecting Yes, multinomial regression will be performed on these categorical variables. The default is No\nPairing variable When auxilary data (1.2.1) is uploaded, this button will show up. It allows users to select variables that contain pairing information in the auxilary data. For example, the matched case-control pair could constitute a categorical variable, where each pair of case and control is given a unique number. Such information can be fed in the modelling of outcome-principal components associations."
  },
  {
    "objectID": "workflow.html#visualization-settings-1",
    "href": "workflow.html#visualization-settings-1",
    "title": "Workflow",
    "section": "\n3.4 Visualization settings",
    "text": "3.4 Visualization settings\nThe orange box is where users can modify their visulization outputs\n\n3.4.1 Basics\n\nConfidence level: The confidence level used when calculating the confidence interval of risk estimate.The default is 0.95.\nWhisker length:The whisker length of confidence intervals in the figures. Change it from 0.1 to 0.5 and see what is changed in Visualizations! The default is 0.1."
  },
  {
    "objectID": "workflow.html#visualizations-2",
    "href": "workflow.html#visualizations-2",
    "title": "Workflow",
    "section": "\n3.5 Visualizations",
    "text": "3.5 Visualizations\nThe green box represents visualization outputs. With omics data from step 1 and outcome data uploaded, upon clicking the Refresh plot button, the risk associations between outcomes and 2 selected components (selected from step 1) are shown. The figure can be downloaded by clicking Download figure."
  },
  {
    "objectID": "workflow.html#visualization-settings-2",
    "href": "workflow.html#visualization-settings-2",
    "title": "Workflow",
    "section": "\n4.1 Visualization settings",
    "text": "4.1 Visualization settings\nThe orange box represents visualization settings. 1. Component/module on first/second axis: The principal component that will be plotted on the x/y-axis. The default is 1/2 (the first/second one).\n\nPlot loadings/correlations/risks: Whether loadings, correlations and risk estimates will be plotted on the visualization. The default is Yes. Upon selecting a No, buttons that are relevant to plot loadings/correlations/risks will not show up.\nLoading/correlation/risk Labels Whether the labels of loadings, correlations and risk estimates will be plotted on the visualization. The default is Yes for correlation and risk labels and No for loading labels.\nLoading/correlation/risk Limits The limits of loadings, correlations and risk estimates on the visualization. The default is set as 1.1 times of the maximum loading value, 1.1 times of the maximum correlation coefficients, 1.1 times of the maximum value for risk estimates’ confidence intervals.\nArrow tip length: The arrow tip length of loadings. The default is 0.02,\nRemove loadings smaller than this value: Loadings below the value will not show up on the plot.\nPlot Scores: Whether the scores will be plotted. The default is No.\nWhisker length: The whisker length of confidence intervals in the figures. Change it from 0.1 to 0.5 and see what is changed in Visualizations! The default is 0.1.\nlog(risk estimate): Users can choose whether they want their risk estimates to be presented as beta coefficients or odds ratios. Two options are provided Yes (useful for odds ratio) and No (useful for beta coefficient). The default is No (useful for beta coefficient)"
  },
  {
    "objectID": "workflow.html#triplot",
    "href": "workflow.html#triplot",
    "title": "Workflow",
    "section": "\n4.2 Triplot",
    "text": "4.2 Triplot\nThe green box represents visualization outputs. With omics data from step 1 and outcome data uploaded, upon clicking the Refresh plot button, the risk associations between outcomes and 2 selected components (selected from step 1) are co-visualized. The figure can be downloaded by clicking Download figure."
  },
  {
    "objectID": "workflow.html#main-settings",
    "href": "workflow.html#main-settings",
    "title": "Workflow",
    "section": "\n5.1 Main settings",
    "text": "5.1 Main settings\n\n5.1.1 Basics\n\nComponent/module on first/second axis: The principal component that will be plotted on the x/y-axis. The default is 1/2 (the first/second one).\nWhat kind of mediation analysis do you want to do Two options are provided, Traditional and Counterfactual. By selecting Traditional, mediation analysis based on the product method will be performed. By selecting Counterfactual mediation analysis, a potential outcome framework provided by the mediation package is used. Here is a summarizarion of these 2 methods in TriplotGUI.\n\n\n\n5.1.2 Emerged\n\nExposure variables: This button will show up upon exposure data is uploaded in step 2. Users can choose which exposures they want to include in their mediation analysis.\nOutcome variables: This button will show up upon outcome data is uploaded in step 2. Users can choose which exposures they want to include in their mediation analysis.\nTreatment/control for exposures When the mediation method is selected as Counterfactual, this button will show up upon exposure data is uploaded in step 2 and exposures for mediation analysis are selected. For each selected exposure variables, users needs to specify two values (for continous exposures) or two classes (for categorical exposures) as treatment and control, respectively\n\n5.1.3 Running mediation analysis\nClick when add new variables/ run with different methods:\nAfter the users select the exposures and outcomes that they want to perform mediation analysis on, unlike previous steps that automatically performs analysis upon uploading data, the users need to click the Do mediation button to start running mediation analysis, due to the time mediation analysis may take. (Mediation analysis will be performed on the each combination of exposure, principal component and outcome).\nUpon clicking, the button will turn grey. A progress bar will then show up at the right bottom corner to remind users if mediation analysis is still running (since it may take sometime). After running mediation analysis, the progress bar will disappear and the button itself will change back to white color.\n\n\n\n\n\n\nNote\n\n\n\n\nThe interface memorizes what mediation analysis has been run (e.g. Using what exposure, what outcome, what method). This means that once you have run mediation analysis on certain exposures and outcomes, even if you unselect an exposure or outcome, their mediation result are not deleted. You can check this by reselecting the unselected exposure/outcome and click Do mediation. You may notice that the mediation analysis is finished rather fast since the result is already there.\nYou can run Counterfactual and Traditional analysis alternatively on the same exposure-outcome pair. The result for different methods will be saved respectively.\nFor a Counterfactual mediation analysis, if you have used exposures-outcomes pairs to run mediation analysis, given specific treatment or control values for exposures, once you use a different treatment or control values for an exposure, all the mediation analysis connecting with that exposure will be rerun and the new results will replaces the corresponding old ones.\n\nCounterfactual mediation analysis may take more time to run Traditional mediation analysis.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nBy selecting exposures, outcomes and specifying mediation method and then clicking Do mediation, the mediation analysis will be run and the results will be saved for the selected exposures, outcomes and mediation methods.\nVisualization will be based on what is selected in the Main settings and set up in the other settings. And the visualization will only show the results (using specific exposures, outcomes and mediation methods) that are saved."
  },
  {
    "objectID": "workflow.html#other-visualization-settings",
    "href": "workflow.html#other-visualization-settings",
    "title": "Workflow",
    "section": "\n5.2 Other visualization settings",
    "text": "5.2 Other visualization settings\n\nMaking triplot with mediation result: The figure will only be further generated when this option is Yes. Upon selecting Yes, all other visualization settings will show up. The default is No.\nPlot medaitions/correlations/risks: Whether mediations, correlations and risk estimates will be plotted on the visualization. The default is Yes. Upon selecting a No, buttons that are relevant to plot mediations/correlations/risks will not show up.\nMediation/correlation/risk Labels Whether the labels of mediations, correlations and risk estimates will be plotted on the visualization. The default is Yes for mediaiton, correlation and risk labels\nMediation/correlation/risk Limits The limits of mediations, correlations and risk estimates on the visualization. The default value is set as the same value in step 3 for correlation and risk Limits. The default value for mediation limits is set as 1.1 times of the maximum value for mediation results’ confidence intervals.\nWhisker length for mediations/risks: The whisker length of confidence intervals in the figure. Change it from 0.1 to 0.5 and see what is changed in Visualizations! The default is 0.1.\nMediation proportion or estimate: Two options are provided, Proportion and Estimate. Upon selecting Proportion, the proportion mediated (The impact of indirect effect among the total effect) will be used as mediation result for visualization. Upon selecting Estimate, the mediation estimates (indirect, direct and total effect) will be used as mediation result for visualization. The default is Proportion\nplot indirect, direct and total effect: Upon selecting Estimate, this checkbox option will show up, providing three choices: IE (indirect effect), DE (direct effect), TE (total effect). Users can chose more than one choices to be visulized on the mediation triplot.\n\nPlot which proportion: Upon selecting Proportion, this checkbox option will show up, providing two choices: IE/(IE+DE) and IE/(abs(IE)+abs(DE))×(IE+DE)/(abs(IE+DE)).\nThe former represents the conventional way of calculating proportion mediated, without considering the situation when the direct and indirect effect goes into opposite directions. This may lead to a proportion bigger than 1.\nThe latter method represents a novel way to calulate proportion mediated. The formula use the absolute value of direct and indirect effect to calculate total effect, which limits the proportion mediated to be within 0-1. The IE and (IE+DE)/(abs(IE+DE)) term in the formula counted in the directionality of indirect and total effect after calulation is perfomed. Users can chose more than one choices to be visulized on the mediation triplot.\n\nlog(risk estimate): Users can choose whether they want their risk estimates to be presented as beta coefficients or odds ratios. Two options are provided, Yes and No . By selecting Yes, odds ratio will be used. By selecting No, beta coefficient will be used. The default is No"
  },
  {
    "objectID": "workflow.html#triplot-with-mediation",
    "href": "workflow.html#triplot-with-mediation",
    "title": "Workflow",
    "section": "\n5.3 Triplot with mediation",
    "text": "5.3 Triplot with mediation\nThe green box represents visualization outputs. With omics data from step 1 and exposure and outcome data from step 2 uploaded and mediation analysis performed, upon clicking the Refresh plot button, the selected exposures’ correlations, selected outcomes risk associations’ and mediation analysis estimates are co-visualized. The figure can be downloaded by clicking Download mediation triplot."
  },
  {
    "objectID": "workflow.html#mediation-barplots",
    "href": "workflow.html#mediation-barplots",
    "title": "Workflow",
    "section": "\n5.4 Mediation barplots",
    "text": "5.4 Mediation barplots\nWe also provide mediation barplots in Step 4, which focus on visualizing the mediation estimates and directions through barplot, using one exposure, one mediator(principal component) and one outcome. The barplot visualization for traditional and counterfactual mediation analysis are separated. \n\n5.4.1 Visualization settings\nThe orange box represents visualization settings. After mediation analyses are run, Exposure/Mediator/Outcome variables that are involved in the mediation analyses will show up in the options for visualization settings. Users can choose one exposure, one mediator and one outcome to make the barplot. \n\n5.4.2 Traditional/Counterfactual mediation barplot\nThe green box represents visualization outputs. Upon clicking the Refresh plot button, the mediation barplot for the selected exposure, mediator and outcome is shown. The figure can be downloaded by clicking Download traditonal/counterfactual mediaiton barplot."
  },
  {
    "objectID": "workflow.html#visualization-settings-4",
    "href": "workflow.html#visualization-settings-4",
    "title": "Workflow",
    "section": "\n6.1 Visualization settings",
    "text": "6.1 Visualization settings\nThe orange box represents visualization settings.\n\n\nWhich PCs you want to remove: All the selected principal components from step 1 will show up as rows in the heatmap. Users can decide which principal components they want to remove from the heatmap visualizations here.\nDo you want correlation coefficients/risk estimates/mediation estimates to be shown: The users can decide what heatmap they want to be shown. The default is Yes for all options.\nWhich correlation/risks you want to move: All the selected exposures and outcomes from step 2 will show up as columns in the heatmap. Users can decide which exposures and outcomes they want to remove from all the heatmap visualizations here.\nWhich type of mediation estimates to be shown: For the mediation heatmap, users can choose what mediation result (direct effect, indirect effect, total effect, two types of proportion mediated) they want to show."
  },
  {
    "objectID": "workflow.html#visualizations-3",
    "href": "workflow.html#visualizations-3",
    "title": "Workflow",
    "section": "\n6.2 Visualizations",
    "text": "6.2 Visualizations\nThe green box represents visualization outputs. Upon clicking the Refresh plot button, the heatmaps for correlations (between selected exposures and principal components), risk associations (between selected outcomes and principal components) and mediations (based on the selected exposures, outcomes, principal components and step 4’s completed mediation analysis results) are shown. The figure can be downloaded by clicking Download figure."
  },
  {
    "objectID": "workflow.html#download-processed-data",
    "href": "workflow.html#download-processed-data",
    "title": "Workflow",
    "section": "\n7.1 Download processed data",
    "text": "7.1 Download processed data\nIn the The darkblue box, users can download processed data in csv format. The processed data of Omics, exposures and outcomes, i.e. data after possible variable removal and class change mentioned in 1.1.2 and 1.2.2, can be viewed and downloaded here.\nIf auxilary data or covariate data are uploaded in step 1 and 2, they processed version can be similarly viewed and downloaded here.\nSeparately, the selected exposures, outcomes and principal components as mediator variables that are used in the mediation analysis in step 4 can be viewed and downloaded here."
  },
  {
    "objectID": "workflow.html#download-exposuress-correlations-outcomes-associations-and-mediation-results",
    "href": "workflow.html#download-exposuress-correlations-outcomes-associations-and-mediation-results",
    "title": "Workflow",
    "section": "\n7.2 Download exposures’s correlations, outcomes’ associations and mediation results",
    "text": "7.2 Download exposures’s correlations, outcomes’ associations and mediation results\nFrom the lightblue box, results from correlations, associations and mediation can be viewed and download here.\n\n\nCorrelation matrix are correlation coefficients between exposures and principal components generated from step 2.\np values of correlations are p values of correlations between exposures and principal components generated from step 2.\n\n\nRisk estimates are beta coefficients between the outcome-principal component association generated from step 2. If users want to use odds ratios, they just simply need to calculate the exponential of the data.\np values of risks are p values of risk estimates between outcomes and principal components generated from step 2.\n\n\nmediation estimates are direct effect, indirect effect and total effect for each combination of exposure, principal component as mediator and outcome generated from step 4.\np values of mediation estimates are p values of direct effect, indirect effect and total effect for each combination of exposure, principal component as mediator and outcome generated from step 4.\n\n\nproportion mediated are two types of proportions ( IE/(IE+DE), IE/(abs(IE)+abs(DE))×(IE+DE)/(abs(IE+DE)) )for each combination of exposure, principal component as mediator and outcome calculated from step 4.\np values of proportion mediated are the p values of IE/(IE+DE) for each combination of exposure, principal component as mediator and outcome calculated from step"
  },
  {
    "objectID": "workflow.html#tpobject",
    "href": "workflow.html#tpobject",
    "title": "Workflow",
    "section": "\n7.3 TPObject",
    "text": "7.3 TPObject\nIn the red box, users can download TPObjects from step 1-4 in rda format. We refer the list that we use to save information and pass them through the steps as TPObject. From step 1, a TPObject is generated, intermediate output and result from each step are added into this TPObject. This aim to facilitate advanced R users to used the information (e.g. intermediate output and results) to perform further analysis and interpretation.\nNote that 2 TPObject can be generated from step 2, seprately, one only use information from exposures and omics, the other only use information form outcomes and omics. The information can be integrated into one TPObject from step 3.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested, please check out the Tutorial(simple), Tutorial(complex), Use Case. to explore what is contained in the TPObject from each step!"
  },
  {
    "objectID": "workflow.html#show-data",
    "href": "workflow.html#show-data",
    "title": "Workflow",
    "section": "\n7.4 Show data",
    "text": "7.4 Show data\nIn the green box,the intermediate data and the generated results can be viewed. Upon clicking View, the corresponding data will show up here.\n\n\nReference"
  }
]